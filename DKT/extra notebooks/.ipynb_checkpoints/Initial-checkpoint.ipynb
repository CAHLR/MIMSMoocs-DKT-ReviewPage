{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_axis = os.path.join(data_dir,'axis_DelftX_AE1110x_2T2015.csv')\n",
    "event_log = os.path.join(data_dir,'DelftX_AE1110x_1T2014-events.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_course_axis = pd.read_csv(course_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_course_axis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# data preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "# changed TimeDistributedDense to TimeDistributed as not supported by version 2.0\n",
    "from keras.layers import Input, Merge, merge, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_time(time):\n",
    "    return datetime.datetime.strptime(time[:-6], '%Y-%m-%dT%H:%M:%S.%f' if '.' in time[:-6] else '%Y-%m-%dT%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MOOC_Data(object):\n",
    "    \"\"\"\n",
    "    Accepts raw edx log of json objects\n",
    "    Converts to pandas dataframe\n",
    "    Has several filtering methods\n",
    "    Ultimate output is sorted_data to be fed to an Abstract_Mapped_To_Indices_Data \n",
    "    \"\"\"\n",
    "    def __init__(self, log_file, course_axis_file):\n",
    "        \"\"\"\n",
    "        Most important attribute is sorted_data\n",
    "        sorted_data is a pandas dataframe with columns from the original \n",
    "        Attributes:\n",
    "        sorted_data: DataFrame of all rows of data sorted by time.\n",
    "                     Columns are: \n",
    "        course_axis: DataFrame of the course axis\n",
    "        problem_check_data: DataFrame of all rows of data that are done by users with a problem check within time constraints        \n",
    "        Reads in json event log and converts to pandas DataFrame\n",
    "        \"\"\"\n",
    "        working_data = []\n",
    "        fail_count = 0\n",
    "        with open(log_file) as data_file:\n",
    "            for line in data_file.readlines():\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    working_data.append(line)\n",
    "                except:\n",
    "                    fail_count+=1\n",
    "                    print(\"Failed to read line:\")\n",
    "                    print(line)\n",
    "                    continue\n",
    "        print(\"Number of failed lines: \", fail_count)\n",
    "        print(\"Length of successfuly read data: \", len(working_data))\n",
    "        print(\"Converting to pandas dataframe now...\")\n",
    "        data_json_str = \"[\" + ','.join(working_data) + \"]\" #Converts json to one large string, since that is what pandas needs\n",
    "        data_df = pd.read_json(data_json_str)\n",
    "#        self.raw_data = data_df\n",
    "        self.sorted_data = data_df.sort_values('time')\n",
    "        self.course_axis = pd.read_csv(course_axis_file)\n",
    "        print(\"Completed loading json file and converted to Pandas DataFrame processing\")\n",
    "\n",
    "    def output_to_disk(self, output_name):\n",
    "        print(\"Outputting sorted dataframe to disk.\")\n",
    "        self.sorted_data.to_csv(output_name)\n",
    "\n",
    "    def filter_data_problem_check(self, minimum_problem_checks = 1):\n",
    "        \"\"\"\n",
    "        Returns a COPY of sorted_data that is filtered to only include users with a problem check.\n",
    "        If you want to overwrite .sorted_data, then reassign it to the returned dataframe of this method\n",
    "        \"\"\"\n",
    "        data_df = self.sorted_data\n",
    "        print(\"Number of rows before filtering for problem check:\", len(data_df))\n",
    "        only_problem_check = data_df[data_df['event_type'] == 'problem_check']\n",
    "        users_with_problem_check = set(only_problem_check.username)\n",
    "        df_only_users_with_problem_check = data_df[data_df['username'].isin(users_with_problem_check)].sort_values('time')\n",
    "        print(\"Number of rows from users with a problem check:\", len(df_only_users_with_problem_check))\n",
    "        return df_only_users_with_problem_check\n",
    "\n",
    "    def filter_data_navigation_only(self):\n",
    "        \"\"\"\n",
    "        Returns a COPY of sorted_data that is filtered to only include rows from navigation events.\n",
    "        \"\"\"\n",
    "        data_df = self.sorted_data\n",
    "        print(\"Number of rows before filtering by only navigation:\", len(data_df))\n",
    "        seq_rows = data_df[data_df['event_type'].isin(['seq_goto', 'seq_next', 'seq_prev'])]\n",
    "        print('seq rows', len(seq_rows))\n",
    "        slash_rows = data_df[data_df['event_type'].str.startswith('/')]\n",
    "        slash_rows = slash_rows[slash_rows['event_type'].str.contains('courseware')]\n",
    "        print('slash rows with courseware', len(slash_rows))\n",
    "        navigation_rows = pd.concat([seq_rows, slash_rows]).sort_values('time')\n",
    "        print(\"Length of navigation rows:\", len(navigation_rows))\n",
    "        return navigation_rows\n",
    "    \n",
    "    ##### Newly added for merging DKT and Behavior\n",
    "    def filter_data_navigation_problem_check_only(self):\n",
    "        \"\"\"\n",
    "        Returns a COPY of sorted_data that is filtered to only include rows from \n",
    "        navigation and problem check events.\n",
    "        \"\"\"\n",
    "        data_df = self.sorted_data\n",
    "        print(\"Number of rows before filtering by only navigation:\", len(data_df))\n",
    "        seq_rows = data_df[data_df['event_type'].isin(['seq_goto', 'seq_next', 'seq_prev'])]\n",
    "        print('seq rows', len(seq_rows))\n",
    "        slash_rows = data_df[data_df['event_type'].str.startswith('/')]\n",
    "        slash_rows = slash_rows[slash_rows['event_type'].str.contains('courseware')]\n",
    "        print('slash rows with courseware', len(slash_rows))\n",
    "        navigation_rows = pd.concat([seq_rows, slash_rows])\n",
    "        print(\"Length of navigation rows:\", len(navigation_rows))\n",
    "        \n",
    "        problem_rows = data_df[data_df['event_type'].isin(['problem_check'])]\n",
    "        print('Problem rows', len(problem_rows))\n",
    "        \n",
    "        nav_problem_rows = pd.concat([navigation_rows, problem_rows]).sort_values('time')\n",
    "        return nav_problem_rows\n",
    "    \n",
    "    ##### Newly added for merging DKT and Behavior    \n",
    "    def add_is_problem_check(self):\n",
    "        \"\"\"\n",
    "        Adds a new column to indicate whether the event_type is problem_check\n",
    "        \"\"\"\n",
    "        def get_is_problem(row):\n",
    "            if row.event_type==\"problem_check\":\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        self.sorted_data[\"is_problem\"] = self.sorted_data.apply(get_is_problem, axis=1)\n",
    "    \n",
    "    def filter_data_by_time(self, earliest_time = datetime.datetime(1900, 1, 1, 0, 0, 0, 000000), latest_time = datetime.datetime(2100, 12, 31, 23, 59, 59, 999999)):\n",
    "        \"\"\"\n",
    "        Returns a COPY of sorted_data that is filtered to only include rows that are between earliest_time and latest_time\n",
    "        \"\"\"\n",
    "        data_df = self.sorted_data\n",
    "        print(\"Length of data before filtering by time:\", len(data_df))\n",
    "        if 'datetime_time' not in data_df.columns:\n",
    "            data_df['datetime_time'] = data_df['time'].apply(process_time)\n",
    "        data_df = data_df[data_df['datetime_time'] <= latest_time]\n",
    "        data_df = data_df[data_df['datetime_time'] >= earliest_time]\n",
    "        print(\"Length of data after filtering by time:\", len(data_df))\n",
    "        return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MOOC_Data_From_Disk(MOOC_Data):\n",
    "    \"\"\"\n",
    "    From Disk means the sorted, processed dataframe is saved to a csv. Reads in a dataframe csv, not an event log.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe_file_name, course_axis_file):\n",
    "        self.sorted_data = pd.read_csv(dataframe_file_name)\n",
    "        self.course_axis = pd.read_csv(course_axis_file)\n",
    "        print(\"Successfully read Dataframe from disk.\", dataframe_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Abstract_Bridge_Between_MOOC_Data_and_Embedding_Indices(object):\n",
    "    \"\"\"\n",
    "    Accepts MOOC_Data object, which contains a sorted_data attribute as well as a course axis\n",
    "    Creates a new internal DataFrame named pre_index_data that contains columns for userid, timestamp, and unique representation of action. Can contain additional columns.\n",
    "    The unique representation of action is what should be converted to indices (for output into embedding layer).\n",
    "    Outputs X y used to train model, along with vocab_size for keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self, MOOC_Data):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.mooc_data = MOOC_Data\n",
    "        self.pre_index_data = pd.DataFrame(columns=['user', 'timestamp', 'is_problem', 'is_correct','unique_representation_of_event'])\n",
    "        self.mappings = None\n",
    "        self.current_full_indices = []\n",
    "        self.current_full_indices_userids = []\n",
    "\n",
    "    @property\n",
    "    def r_mappings(self):\n",
    "        r_mapping = {v: k for k, v in self.mappings.items()}\n",
    "        return r_mapping\n",
    "\n",
    "    def populate_pre_index(self):\n",
    "        \"\"\"\n",
    "        Abstract Method\n",
    "        \"\"\"\n",
    "        print(\"Warning: Need to implement populate_pre_index\")\n",
    "        return -1\n",
    "\n",
    "    def expose_x_y(self):\n",
    "        \"\"\"\n",
    "        Abstract Method\n",
    "        Returns X and y numpy arrays that will be fed into Keras Model\n",
    "        \"\"\"\n",
    "        print(\"WARNING: NEED TO IMPLEMENT EXPOSE_X_Y\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_vertical(sequential, chapter, vertical=-1):\n",
    "    return '/' + chapter + '/' + sequential + '/' + str(vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vertical_Output(Abstract_Bridge_Between_MOOC_Data_and_Embedding_Indices):\n",
    "    def __init__(self, MOOC_Data):\n",
    "        Abstract_Bridge_Between_MOOC_Data_and_Embedding_Indices.__init__(self, MOOC_Data)\n",
    "\n",
    "    def populate_mappings_based_on_verticals_in_course_axis(self):\n",
    "        course_axis = self.mooc_data.course_axis\n",
    "        mapping = {1: 'pre_start'}\n",
    "        current_index = 2\n",
    "        for action in list(course_axis[course_axis.category=='vertical'].path):\n",
    "            mapping[current_index] = action\n",
    "            current_index += 1\n",
    "        r_mapping = {v: k for k, v in mapping.items()}\n",
    "        self.mappings = mapping\n",
    "\n",
    "    def populate_time_spent_in_pre_index_data(self):\n",
    "        if not self.pre_index_data:\n",
    "            print(\"Warning: pre_index_data is empty.\")\n",
    "            return\n",
    "        groupedbyuser = self.pre_index_data.groupby('user')\n",
    "        for user, data in groupedbyuser:\n",
    "            for row in data.iterrows():\n",
    "                123412341234\n",
    "                print(\"not yet implemented\")\n",
    "    \n",
    "    def create_full_indices_based_on_pre_index_data_ignoring_time_spent(self):\n",
    "        \"\"\"\n",
    "        Returns pre_index_data mapped to indices (list of lists), as well as corresponding list of user ids\n",
    "        \"\"\"\n",
    "        if not self.mappings:\n",
    "            print(\"Error: Mappings not yet populates\")\n",
    "            return -1\n",
    "        list_of_indices = []\n",
    "        list_of_indices_userids = []\n",
    "        grouped_by_user = self.pre_index_data.groupby('user')\n",
    "        for user_id, data in grouped_by_user:\n",
    "            list_of_indices_userids.append(user_id)\n",
    "            current_user_indices = []\n",
    "            full_indices = [self.r_mappings[url] for url in list(data.unique_representation_of_event)]\n",
    "            list_of_indices.append(full_indices)\n",
    "        return list_of_indices, list_of_indices_userids\n",
    "        \n",
    "    def prepend_1_to_current_full_indices(self):\n",
    "        \"\"\"\n",
    "        MUTATES self.current_full_indices such that a 1 is prepended to all lists\n",
    "        Will not prepend a 1 if there is already a 1 at the start\n",
    "        \"\"\"\n",
    "        for seq in self.current_full_indices:\n",
    "            if seq[0] == 1:\n",
    "                continue\n",
    "            else:\n",
    "                seq.reverse()\n",
    "                seq.append(1)\n",
    "                seq.reverse()\n",
    "                continue\n",
    "\n",
    "        \n",
    "    def remove_contiguous_repeats_from_pre_index_data(self, keep_highest_time_spent = True):\n",
    "        \"\"\"\n",
    "        Returns a copy of pre_index_data where contiguous repeats are removed\n",
    "        \"\"\"\n",
    "        grouped_by_user = self.pre_index_data.groupby('user')\n",
    "        data_to_dataframe = [] #will be 5 columns, matching pre_index_data\n",
    "        for user_id, data in grouped_by_user:\n",
    "            previous_element = None\n",
    "            for row in data.iterrows():\n",
    "                index = row[0]\n",
    "                values = row[1]\n",
    "                u = values.user\n",
    "                t = values.timestamp\n",
    "                p = values.is_problem\n",
    "                c = values.is_correct\n",
    "                url = values.unique_representation_of_event\n",
    "                t_spent = values.time_spent\n",
    "                \n",
    "                if p==1:\n",
    "                    data_to_dataframe.append([u, t, p, c, url, t_spent])\n",
    "                    # for problem_check data removing contiguous repeats is not required. \n",
    "                    continue\n",
    "                \n",
    "                if not previous_element:\n",
    "                    previous_element = url\n",
    "                    data_to_dataframe.append([u, t, p, c, url, t_spent])\n",
    "                    continue\n",
    "                else:\n",
    "                    if url == previous_element:\n",
    "                        if keep_highest_time_spent:\n",
    "                            currently_recorded_time_spent = data_to_dataframe[-1][-1]\n",
    "                            if isinstance(t_spent, str):\n",
    "                                # t_spent is therefore 'endtime'\n",
    "                                data_to_dataframe[-1][-1] = t_spent\n",
    "                            elif t_spent > currently_recorded_time_spent:\n",
    "                                data_to_dataframe[-1][-1] = t_spent\n",
    "                            else:\n",
    "                                continue\n",
    "                        continue\n",
    "                    else:\n",
    "                        previous_element = url\n",
    "                        data_to_dataframe.append([u, t, p, c, url, t_spent])\n",
    "        temp_df = pd.DataFrame(data_to_dataframe, columns = self.pre_index_data.columns)\n",
    "        return temp_df\n",
    "\n",
    "    def populate_time_spent(self):\n",
    "        \"\"\"\n",
    "        Returns a copy of self.pre_index_data with the time_spent column populated with integer between 0 and 3\n",
    "        \"\"\"\n",
    "        pre_index_data = self.pre_index_data\n",
    "        grouped_by_user = pre_index_data.groupby('user')\n",
    "        data_to_append = []\n",
    "        for user_id, data in grouped_by_user:\n",
    "            new_time_sequence = []\n",
    "            timestamps = [process_time(elem) for elem in list(data.timestamp)]\n",
    "            for i in range(0, len(timestamps) - 1):\n",
    "                current_time = timestamps[i]\n",
    "                next_time = timestamps[i+1]\n",
    "                second_difference = (next_time - current_time).total_seconds()\n",
    "                new_time_sequence.append(second_difference)\n",
    "            new_time_sequence.append('endtime')\n",
    "\n",
    "            i = 0\n",
    "            for row in data.iterrows():\n",
    "                index = row[0]\n",
    "                values = row[1]\n",
    "                user = values.user\n",
    "                timestamp = timestamps[i]\n",
    "                is_problem = values.is_problem\n",
    "                is_correct = values.is_correct\n",
    "                rep = values.unique_representation_of_event\n",
    "                time_spent = new_time_sequence[i]\n",
    "                data_to_append.append([user, timestamp, is_problem, is_correct, rep, time_spent])\n",
    "                i+=1\n",
    "\n",
    "        temp_df = pd.DataFrame(data_to_append, columns = ['user', 'timestamp', 'is_problem', 'is_correct', 'unique_representation_of_event', 'time_spent'])\n",
    "        return temp_df\n",
    "\n",
    "\n",
    "    def populate_pre_index_data(self):\n",
    "        \"\"\"\n",
    "        Populates self.pre_index_data with a dataframe that includes a vertical_url column,\n",
    "        such that navigational data is resolved to a specific course URL\n",
    "        \"\"\"\n",
    "        course_axis = self.mooc_data.course_axis\n",
    "        ordered_vertical_paths = list(course_axis[course_axis.category == 'vertical'].path)\n",
    "        sequential_paths = list(course_axis[course_axis.category == 'sequential'].path)\n",
    "        chapter_paths = list(course_axis[course_axis.category == 'chapter'].path)\n",
    "        chapter_set =  set([elem[1:] for elem in list(course_axis[course_axis.category == 'chapter'].path)])\n",
    "        all_paths = list(course_axis.path)\n",
    "        seq_counts = [0, 0, 0]\n",
    "        every_category = [0, 0, 0, 0]\n",
    "        prev_next_conversions = [0, 0]\n",
    "        data_to_append = [] #should be 5 element per row, with columns user, timestamp, vertical_url. time_spent\n",
    "        #will eventually append data_to_append to the pre_index_data dataframe, to eventually convert to x, y in expose_x_y\n",
    "        sequential_to_chap = {}\n",
    "        prev_next_conversions = [0, 0]\n",
    "        for path in sequential_paths:\n",
    "            seq = path.split('/')[-1]\n",
    "            chap = path.split('/')[-2]\n",
    "            sequential_to_chap[seq] = chap\n",
    "            \n",
    "        grouped_by_user = self.mooc_data.sorted_data.groupby('username')\n",
    "        for user_id, data in grouped_by_user:\n",
    "            chapter_location = {} #key is chapter, value is [sequential, vertical]\n",
    "            sequential_location = {}\n",
    "            for chapter in chapter_set:\n",
    "                for p in sequential_paths:\n",
    "                    if chapter in p:\n",
    "                        chapter_location[chapter] = [p.split('/')[-1], 1]\n",
    "                        break            \n",
    "            for sequential in sequential_to_chap:\n",
    "                sequential_location[sequential] = 1\n",
    "            for row in data.iterrows():\n",
    "                index = row[0]\n",
    "                values = row[1]\n",
    "                et = values.event_type\n",
    "                is_seq = False\n",
    "                \n",
    "                seq_events = ['seq_goto', 'seq_next', 'seq_prev']\n",
    "                for event in seq_events:\n",
    "                    if event in et:\n",
    "                        is_seq = True\n",
    "                        break\n",
    "                # added to include problem type event\n",
    "                # for problem event the problem_id is taken as the unique representation\n",
    "                if values.is_problem == 1:\n",
    "                    new_action = values.event['problem_id'].split(\"@\")[-1] #Get the problem id\n",
    "                    correctness = values.event['success']\n",
    "                    \n",
    "                elif is_seq:\n",
    "                    e = json.loads(values.event)\n",
    "                    action = str(e['new']) + '_' + str(et) + '_' + e['id'].split('/')[-1]\n",
    "                    split = action.split('_')\n",
    "                    new_vertical = int(split[0])\n",
    "                    split = action.split('@')\n",
    "                    seq_id = split[-1]\n",
    "                    if seq_id not in sequential_to_chap:\n",
    "                        continue\n",
    "                    chap_id = sequential_to_chap[seq_id]\n",
    "                    new_action = construct_vertical(seq_id, chap_id, new_vertical)\n",
    "                    test_string = new_action #basically testing if the new potential vertical is an actual possible vertical according to course axis\n",
    "                    if test_string not in all_paths:\n",
    "                        if event == 'seq_prev':\n",
    "                            corresponding_vertical_index = ordered_vertical_paths.index(construct_vertical(seq_id,chap_id,new_vertical+1))\n",
    "                            new_action = ordered_vertical_paths[corresponding_vertical_index-1]\n",
    "                            split = new_action.split('/')\n",
    "                            new_vertical = split[-1]\n",
    "                            seq_id = split[2]\n",
    "                            chap_id = split[1]\n",
    "                            prev_next_conversions[0]+=1\n",
    "                        elif event == 'seq_next':\n",
    "                            corresponding_vertical_index = ordered_vertical_paths.index(construct_vertical(seq_id,chap_id,new_vertical-1))\n",
    "                            new_action = ordered_vertical_paths[corresponding_vertical_index+1]\n",
    "                            split = new_action.split('/')\n",
    "                            new_vertical = split[-1]\n",
    "                            seq_id = split[2]\n",
    "                            chap_id = split[1]\n",
    "                            prev_next_conversions[1]+=1\n",
    "                        else:\n",
    "                            raise Exception()\n",
    "\n",
    "                    sequential_location[seq_id] = new_vertical\n",
    "                    if event == 'seq_prev':\n",
    "                        seq_counts[0] += 1\n",
    "                    elif event == 'seq_next':\n",
    "                        seq_counts[1] += 1\n",
    "                    elif event == 'seq_goto':\n",
    "                        seq_counts[2] += 1\n",
    "                    else:\n",
    "                        raise Exception()\n",
    "                    every_category[0] += 1\n",
    "                    \n",
    "                else:\n",
    "                    action = et\n",
    "                    split = action.split('/')\n",
    "                    last_elem = split[-1]\n",
    "                    if len(last_elem) == 1 or len(last_elem) == 2:\n",
    "                        seq_id = split[-2]\n",
    "                        if seq_id not in sequential_to_chap:\n",
    "                            continue\n",
    "                        chap_id = split[-3]\n",
    "                        if chap_id not in chapter_location:\n",
    "                            continue\n",
    "                        try:\n",
    "                            new_vertical = int(last_elem)\n",
    "                        except:\n",
    "                            new_vertical = last_elem\n",
    "                        new_action = construct_vertical(seq_id, chap_id, new_vertical)\n",
    "                        test_string = new_action\n",
    "                        if test_string not in all_paths:\n",
    "                            print(\"Nonsense vertical: \", test_string, \"Debug code: 1\")\n",
    "                            new_vertical = 1 #resolve nonsense direct vertical to vertical 1\n",
    "                            new_action = construct_vertical(seq_id, chap_id, new_vertical)\n",
    "                        sequential_location[seq_id] = new_vertical\n",
    "                        every_category[1] += 1\n",
    "                    elif split[-3] == 'courseware':\n",
    "                        chap_id = split[-2]\n",
    "                        if chap_id not in chapter_location:\n",
    "                            print(\"couldn't find this chapter in course axis:\", action)\n",
    "                            continue\n",
    "                        seq_id = chapter_location[chap_id][0]\n",
    "                        new_vertical = int(chapter_location[chap_id][1])\n",
    "                        new_action = construct_vertical(seq_id, chap_id, int(new_vertical))\n",
    "                        test_string = new_action\n",
    "                        if test_string not in all_paths:\n",
    "                            print(\"Nonsense vertical: \", test_string, \"Debug code: 2\")\n",
    "                            new_vertical = 1 #resolve nonsense direct vertical to vertical 1\n",
    "                            new_action = construct_vertical(seq_id, chap_id, new_vertical)\n",
    "                        every_category[2] += 1\n",
    "                    #is a chapter event with no related sequential\n",
    "                    else:\n",
    "                    #is a sequential event with no vertical\n",
    "                        seq_id = split[-2]\n",
    "                        chap_id = split[-3]\n",
    "                        if seq_id not in sequential_location:\n",
    "                            continue\n",
    "                        if chap_id not in chapter_location:\n",
    "                            continue\n",
    "                        new_vertical = sequential_location[seq_id]\n",
    "                        new_action = construct_vertical(seq_id, chap_id, int(new_vertical))\n",
    "                        test_string = new_action\n",
    "                        if test_string not in all_paths:\n",
    "                            print(\"Nonsense vertical: \", test_string, \"Debug code: 3\")\n",
    "                            new_vertical = 1 #resolve nonsense direct vertical to vertical 1\n",
    "                            new_action = construct_vertical(seq_id, chap_id, new_vertical)\n",
    "                        every_category[3] += 1\n",
    "                        \n",
    "                chapter_location[chap_id] = [seq_id, new_vertical]\n",
    "                time = values.time\n",
    "                # for non problem_check event the correctness column value is kept as None.\n",
    "                is_problem = values.is_problem\n",
    "                if is_problem == 0:\n",
    "                    correctness = None\n",
    "                data_to_append.append([user_id, time, is_problem, correctness, new_action, 'not yet calculated'])\n",
    "        temp_df = pd.DataFrame(data_to_append, columns = ['user', 'timestamp', 'is_problem', 'is_correct', 'unique_representation_of_event', 'time_spent'])\n",
    "        self.pre_index_data = temp_df\n",
    "        print('seq_prev, seq_next, seq_goto counts:', seq_counts)\n",
    "        print('seq events, direct vertical in url, only chapter in url, chapter and sequential in url counts:', every_category)\n",
    "        print('times seq_prev was used on first vertical in sequence, ditto for seq_next counts:',prev_next_conversions)\n",
    "    \n",
    "    def expose_x_y(self, max_len = 5000, min_len = 3):\n",
    "        \"\"\"\n",
    "        Returns X, y numpy arrays based on current_full_indices\n",
    "        \"\"\"\n",
    "        x_windows = [seq[:-1] for seq in self.current_full_indices if len(seq) >= min_len]\n",
    "        y_windows = [seq[1:] for seq in self.current_full_indices if len(seq) >= min_len]\n",
    "        X = sequence.pad_sequences(x_windows, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        padded_y_windows = sequence.pad_sequences(y_windows, maxlen=max_len, padding = 'post', truncating = 'post')\n",
    "        self.padded_y_windows = padded_y_windows\n",
    "        y = np.zeros((len(padded_y_windows), max_len, len(self.mappings)), dtype = np.bool)\n",
    "        for i, output in enumerate(padded_y_windows):\n",
    "            for t, resource_index in enumerate(output):\n",
    "                if resource_index == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    y[int(i), int(t), int(resource_index)-1] = 1\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MOOC_Keras_Model(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.keras_model = None\n",
    "        self.X = None\n",
    "        self.padded_y_windows = None\n",
    "        self.y = None\n",
    "        self.model_params = None\n",
    "        self.model_histories = []\n",
    "        self.embedding_vocab_size = None\n",
    "        self.best_epoch = None\n",
    "        self.previous_val_loss = []\n",
    "\n",
    "    def import_data(self, X, y, additional_params = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "    \n",
    "    def set_model_name(self, name):\n",
    "        if not self.model_params:\n",
    "            print(\"WARNING: Create LSTM model before setting model name.\")\n",
    "            return -1\n",
    "        self.model_name = name + self.model_params_to_string\n",
    "\n",
    "    @property\n",
    "    def model_params_to_string(self):\n",
    "        mp = self.model_params\n",
    "        return '_' + str(mp['layers']) + '_' + str(mp['lrate']) + '_' + str(mp['hidden_size']) + '_' + str(mp['opt']) + '_' + str(mp['e_size']) + '_' + str( mp['output_dim']) + '_' + str(mp['input_len']) + '_' + str(mp['embedding_vocab_size'])\n",
    "\n",
    "    def create_basic_lstm_model(self, layers, lrate, hidden_size, opt, e_size, output_dim, input_len, embedding_vocab_size):\n",
    "        \"\"\"\n",
    "        Returns a LSTM model\n",
    "        \"\"\"\n",
    "        print('building a functional API model')\n",
    "\n",
    "        self.model_params = {'layers': layers, 'lrate': lrate, 'hidden_size': hidden_size, 'opt': opt, 'e_size': e_size, 'output_dim': output_dim, 'input_len': input_len, 'embedding_vocab_size': embedding_vocab_size}\n",
    "\n",
    "        main_input = Input(shape=(input_len,), name = 'main_input', dtype='int32')\n",
    "        x = Embedding(output_dim = e_size, input_dim = embedding_vocab_size+1, input_length = input_len, mask_zero = True)(main_input)\n",
    "        for i in range(layers):\n",
    "            print(\"adding layer \" + str(i))\n",
    "            x = LSTM(hidden_size, dropout_W = 0.2, return_sequences = True)(x)\n",
    "        main_loss = TimeDistributed(Dense(output_dim, activation='softmax'))(x)\n",
    "        model = Model(input=[main_input], output = [main_loss])\n",
    "        opt = opt(lr = lrate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def load_keras_weights_from_disk(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    def early_stopping_model_fit(self, train_x, train_y, validation_data, epoch_limit = 200, loss_nonimprove_limit = 3, batch_size = 64, save_models_to_folder = None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        early_stopping_met = False\n",
    "        for i in range(epoch_limit):\n",
    "            print(\"epoch:\", i)\n",
    "            current_history = self.keras_model.fit(train_x, train_y, batch_size = batch_size, nb_epoch = 1, validation_data = validation_data)\n",
    "            current_history = current_history.history\n",
    "            validation_loss = current_history['val_loss'][0]\n",
    "            validation_accuracy_dictionary = self.compute_validation_accuracy(validation_data, b_s = batch_size)\n",
    "            average_of_average_accuracy = np.mean(validation_accuracy_dictionary['averages'])\n",
    "            accuracy = validation_accuracy_dictionary['accuracy']\n",
    "            self.previous_val_loss.append(validation_loss)\n",
    "            if len(self.previous_val_loss) > loss_nonimprove_limit:\n",
    "                min_val_loss = min(self.previous_val_loss)\n",
    "                recent_losses = self.previous_val_loss[-loss_nonimprove_limit-1:]\n",
    "                print(recent_losses)\n",
    "                if min(recent_losses) > min_val_loss:\n",
    "                    early_stopping_met = True\n",
    "                if validation_loss == min_val_loss:\n",
    "                    self.best_epoch = i\n",
    "                    self.best_average_of_average_accuracy = average_of_average_accuracy\n",
    "                    self.best_accuracy = accuracy                    \n",
    "            if early_stopping_met:\n",
    "                print(\"Early stopping reached.\")\n",
    "                print(\"Best epoch according to validation loss:\", self.best_epoch)\n",
    "                print(\"Best epoch's accuracy:\", self.best_accuracy)\n",
    "                print(\"Best epoch's average accuracy:\", self.best_average_of_average_accuracy)\n",
    "                return\n",
    "\n",
    "    def compute_validation_accuracy(self, validation_data, b_s = 64):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        validation_x = validation_data[0]\n",
    "        validation_y = validation_data[1]\n",
    "        just_x_indices = validation_x\n",
    "        if isinstance(validation_x, list):\n",
    "            just_x_indices = validation_x[0]\n",
    "        predictions = self.keras_model.predict(validation_x, batch_size = b_s)\n",
    "\n",
    "        per_student_accuracies = []\n",
    "        total_correct_predictions = 0\n",
    "        total_incorrect_predictions = 0\n",
    "\n",
    "        for student_sequence_index, current_x in enumerate(just_x_indices):\n",
    "            corresponding_predictions = list(predictions[student_sequence_index])\n",
    "            corresponding_answers = list(validation_y[student_sequence_index])\n",
    "            current_student_correct = 0\n",
    "            current_student_incorrect = 0\n",
    "            for prediction_index in range(len(current_x)):\n",
    "                current_value = current_x[prediction_index]\n",
    "                if current_value == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    current_softmax = list(corresponding_predictions[prediction_index]) #softmax probability distribution\n",
    "                    best_prediction = current_softmax.index(max(current_softmax)) + 1\n",
    "                    correct_answer = list(corresponding_answers[prediction_index]).index(1) + 1\n",
    "                    is_correct = best_prediction == correct_answer\n",
    "                    \n",
    "                    if is_correct:\n",
    "                        current_student_correct += 1\n",
    "                        total_correct_predictions += 1\n",
    "                    else:\n",
    "                        current_student_incorrect +=1\n",
    "                        total_incorrect_predictions += 1\n",
    "            acc = float(current_student_correct) / (current_student_incorrect + current_student_correct)\n",
    "            per_student_accuracies.append(acc)\n",
    "        total_val_acc = float(total_correct_predictions) / (total_correct_predictions + total_incorrect_predictions)\n",
    "        print(\"Total validation accuracy:\", total_val_acc)\n",
    "        print(\"Average accuracy:\", np.mean(per_student_accuracies))\n",
    "        return_dict = {}\n",
    "        return_dict['accuracy'] = total_val_acc\n",
    "        return_dict['averages'] = per_student_accuracies\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_log_file = \"../data/DelftX_AE1110x_2T2015-events.log\"\n",
    "sample_course_axis = \"../data/axis_DelftX_AE1110x_2T2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reading in sample log file:', '../data/DelftX_AE1110x_2T2015-events.log')\n",
      "Failed to read line:\n",
      "{\"username\": \"username_219376334\", \"context\": {\"course_id\": \"course-v1:DelftX+AE1110x+2T2015\", \"course_user_tags\": {}, \"user_id\": 219376334, \"org_id\": \"DelftX\"}, \"event_source\": \"server\", \"name\": \"edx.forum.comment.created\", \"nonInteraction\": 1, \"agent\": \"Mozilla/5.0 (Linux; Android 5.0; SAMSUNG SM-N9005 Build/LRX21V) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/2.1 Chrome/34.0.1847.76 Mobile Safari/537.36\", \"label\": \"course-v1:DelftX+AE1110x+2T2015\", \"session\": \"dee493e85088b611c223a1637348e56b\", \"accept_language\": \"en-GB,en;q=0.8,en-US;q=0.6,en;q=0.4\", \"time\": \"2015-11-29T16:53:42.486482+00:00\", \"event_type\": \"edx.forum.comment.created\", \"augmented\": {\"country_code\": \"EG\"}, \"event\": {\"body\": \"Thanks a lot \\U0001f60a\\U0001f60a\\U0001f60a\", \"commentable_id\": \"ec3f7d0dc9934fb8b22d1154321068cd\", \"truncated\": false, \"discussion\": {\"id\": \"56479e081dfe59735c00025c\"}, \"options\": {\"followed\": false}, \"user_forums_roles\": [\"Student\"], \"id\": \"565b2d959da26a9f5200060c\", \"user_course_roles\": [], \"category_id\": \"ec3f7d0dc9934fb8b22d1154321068cd\", \"response\": {\"id\": \"5649c5a21dfe599a55000313\"}, \"category_name\": \"Week 5 / Discussion lecture 3b\"}}\n",
      "\n",
      "Failed to read line:\n",
      "{\"username\": \"username_607393346\", \"context\": {\"course_id\": \"course-v1:DelftX+AE1110x+2T2015\", \"course_user_tags\": {}, \"user_id\": 607393346, \"org_id\": \"DelftX\"}, \"event_source\": \"server\", \"name\": \"edx.forum.comment.created\", \"nonInteraction\": 1, \"agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1\", \"label\": \"course-v1:DelftX+AE1110x+2T2015\", \"session\": \"03eb7017739a3136d4e78f63fb3f98cf\", \"accept_language\": \"es-es\", \"time\": \"2015-12-02T13:46:22.503904+00:00\", \"event_type\": \"edx.forum.comment.created\", \"augmented\": {\"country_code\": \"ES\"}, \"event\": {\"body\": \"Yeah, that's what I thought. I suppose A380 wings must be super, super critical \\U0001f600 \", \"commentable_id\": \"7360e3512b8f46c8ab27829a9f1550e6\", \"truncated\": false, \"discussion\": {\"id\": \"565e0afd4ed74967fc000030\"}, \"id\": \"565ef62d4ed749ca69000097\", \"user_forums_roles\": [\"Student\"], \"user_course_roles\": [], \"response\": {\"id\": \"565ed1db88f7565ac500007d\"}, \"category_id\": \"7360e3512b8f46c8ab27829a9f1550e6\", \"options\": {\"followed\": false}, \"category_name\": \"Week 9 / Discussion Lecture 4d\"}}\n",
      "\n",
      "Failed to read line:\n",
      "{\"username\": \"username_254023324\", \"context\": {\"course_id\": \"course-v1:DelftX+AE1110x+2T2015\", \"course_user_tags\": {}, \"user_id\": 254023324, \"org_id\": \"DelftX\"}, \"event_source\": \"server\", \"name\": \"edx.forum.thread.created\", \"nonInteraction\": 1, \"agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 9_2 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13C75 Safari/601.1\", \"label\": \"course-v1:DelftX+AE1110x+2T2015\", \"session\": \"71c037e4fe36897f1e3b784d05135401\", \"accept_language\": \"en-us\", \"time\": \"2015-12-13T21:30:16.912358+00:00\", \"event_type\": \"edx.forum.thread.created\", \"augmented\": {\"country_code\": \"US\"}, \"event\": {\"body\": \"Hello fellow students, I'm <<FULLNAME>> from India. I currently live in Hartford, Connecticut, USA. I joined this class because I always liked physics and engineering so I wanted to check this course out. I'm a little young (14 years) so pleas help me out if you can. Thanks\\U0001f600.\", \"anonymous_to_peers\": false, \"category_id\": \"2ad017b726954ea29595e8a5f468f95e\", \"title\": \"Introduction: I'm <<FULLNAME>>\", \"id\": \"566de36808f9a5873000027d\", \"truncated\": false, \"thread_type\": \"discussion\", \"commentable_id\": \"2ad017b726954ea29595e8a5f468f95e\", \"user_forums_roles\": [\"Student\"], \"anonymous\": false, \"user_course_roles\": [], \"group_id\": null, \"options\": {\"followed\": true}, \"category_name\": \"Week 0 / The \\\"Say hello\\\" forum\"}}\n",
      "\n",
      "Failed to read line:\n",
      "{\"username\": \"username_731276363\", \"context\": {\"course_id\": \"course-v1:DelftX+AE1110x+2T2015\", \"course_user_tags\": {}, \"user_id\": 731276363, \"org_id\": \"DelftX\"}, \"event_source\": \"server\", \"name\": \"edx.forum.thread.created\", \"agent\": \"Mozilla/5.0 (iPad; CPU OS 9_2_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13D15 Safari/601.1\", \"event\": {\"body\": \"I'm here to learn more about aeronautical engineering. I hope to find some educational material within this course that will aid me in the coming years. Fun Fact: As of 2016, I am 14 years old (probably a little younger than most people on edX \\U0001f602)  I can't wait to get started \\U0001f601\\u270c\\U0001f3fb\\ufe0f\", \"anonymous_to_peers\": false, \"category_id\": \"2ad017b726954ea29595e8a5f468f95e\", \"title\": \"Introduction: <<FULLNAME>>, Australia, (Melbourne, Victoria)\", \"id\": \"57230614d11155057e000454\", \"truncated\": false, \"thread_type\": \"discussion\", \"commentable_id\": \"2ad017b726954ea29595e8a5f468f95e\", \"user_forums_roles\": [\"Student\"], \"anonymous\": false, \"user_course_roles\": [], \"group_id\": null, \"options\": {\"followed\": true}, \"category_name\": \"Week 0 / The \\\"Say hello\\\" forum\"}, \"session\": \"d81d74c946c7c47a74d2e0dc02b39ab9\", \"accept_language\": \"en-au\", \"time\": \"2016-04-29T06:58:28.690355+00:00\", \"augmented\": {\"country_code\": \"AU\"}, \"event_type\": \"edx.forum.thread.created\"}\n",
      "\n",
      "('Number of failed lines: ', 4)\n",
      "('Length of successfuly read data: ', 7942207)\n",
      "Converting to pandas dataframe now...\n",
      "Completed loading json file and converted to Pandas DataFrame processing\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading in sample log file:\", sample_log_file)\n",
    "# creates a MOOC_Data object\n",
    "my_data = MOOC_Data(sample_log_file, sample_course_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of rows before filtering for problem check:', 7942207)\n",
      "('Number of rows from users with a problem check:', 7117743)\n"
     ]
    }
   ],
   "source": [
    "my_data.sorted_data = my_data.filter_data_problem_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of rows before filtering by only navigation:', 7117743)\n",
      "('seq rows', 714439)\n",
      "('slash rows with courseware', 408416)\n",
      "('Length of navigation rows:', 1122855)\n",
      "('Problem rows', 683319)\n"
     ]
    }
   ],
   "source": [
    "my_data.sorted_data = my_data.filter_data_navigation_problem_check_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data.add_is_problem_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of data before filtering by time:', 1806174)\n",
      "('Length of data after filtering by time:', 1806174)\n"
     ]
    }
   ],
   "source": [
    "my_data.sorted_data = my_data.filter_data_by_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_verticals = Vertical_Output(my_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806174\n"
     ]
    }
   ],
   "source": [
    "print(len(my_verticals.mooc_data.sorted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_verticals.populate_mappings_based_on_verticals_in_course_axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped_by_user = my_verticals.mooc_data.sorted_data.groupby('username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for user,data in grouped_by_user:\n",
    "#     for row in data.iterrows():\n",
    "#         index = row[0]\n",
    "#         values = row[1]\n",
    "#         if values.event_type==\"problem_check\":\n",
    "#             print(values.is_problem)\n",
    "#             print(values.event['problem_id'].split(\"@\")[-1])\n",
    "#             break\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nonsense vertical: ', u'/669e9115be17498a9fa0cc5cee0cb78f/3ca3d5812aef405a8aaa1f296d4985aa/]', 'Debug code: 1')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/949231a21e6b4../')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/949231a21e6b4../')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/6046759339d2../')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/6046759339d2../')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/6046759339d2../')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/6046759339d2../')\n",
      "('Nonsense vertical: ', u'/669e9115be17498a9fa0cc5cee0cb78f/3ca3d5812aef405a8aaa1f296d4985aa/v', 'Debug code: 1')\n",
      "(\"couldn't find this chapter in course axis:\", u'/courses/course-v1:DelftX+AE1110x+2T2015/courseware/669e9115be17498a9fa0/')\n",
      "('seq_prev, seq_next, seq_goto counts:', [20541, 152196, 541702])\n",
      "('seq events, direct vertical in url, only chapter in url, chapter and sequential in url counts:', [714439, 2579, 87705, 317730])\n",
      "('times seq_prev was used on first vertical in sequence, ditto for seq_next counts:', [97, 665])\n"
     ]
    }
   ],
   "source": [
    "my_verticals.populate_pre_index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_verticals.pre_index_data = my_verticals.populate_time_spent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_verticals.pre_index_data = my_verticals.remove_contiguous_repeats_from_pre_index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_verticals.pre_index_data[\"time_spent\"] = pd.to_numeric(my_verticals.pre_index_data['time_spent'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_verticals.pre_index_data[\"time_spent\"].fillna(my_verticals.pre_index_data[\"time_spent\"].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_verticals.pre_index_data.to_csv(\"pre_index_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_groups(group,limit=5000):\n",
    "    group = group.sort_values(by='timestamp')\n",
    "    indices=list(range(1,len(group[\"timestamp\"])+1))\n",
    "#     if len(group[\"timestamp\"]) > limit:\n",
    "#         indices[limit:] = [0]* (len(group[\"timestamp\"])-limit)\n",
    "    group['timesteps'] = indices\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_userdf = my_verticals.pre_index_data.groupby('user').apply(process_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_event_sequence = grouped_userdf.pivot(index= 'user', columns = 'timesteps', values = 'unique_representation_of_event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_events = my_verticals.pre_index_data.unique_representation_of_event.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_events_dict = {value:key+1 for key,value in enumerate(unique_events)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_user_event_sequence = user_event_sequence.iloc[:,:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistment_df = trunc_user_event_sequence.replace(unique_events_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO : Change None to 0 in assistment df\n",
    "assistment_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assistment_df = assistment_df.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change correct to 1 and incorrect to 0\n",
    "grouped_userdf[\"is_correct\"].replace({\"correct\":1,\"incorrect\":0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_sequence = grouped_userdf.pivot(index= 'user', columns = 'timesteps', values = 'is_correct')\n",
    "#TO DO: truncate to timesteps\n",
    "trunc_response_sequence = response_sequence.iloc[:,:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : Prepare a new column called effort: Populate as if <300 =0 else =1\n",
    "# Pivot timesteps to have effort as value\n",
    "# truncate to max_timesteps\n",
    "effort_threshold = 300\n",
    "mask_0 = grouped_userdf[\"time_spent\"]<effort_threshold\n",
    "column_name = \"effort\"\n",
    "grouped_userdf.loc[mask_0,column_name] = 0\n",
    "\n",
    "grouped_userdf.effort.fillna(1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effort_sequence = grouped_userdf.pivot(index= 'user', columns = 'timesteps', values = 'effort')\n",
    "#TO DO: truncate to timesteps\n",
    "trunc_effort_sequence = effort_sequence.iloc[:,:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat response and effort to get the response_effort data\n",
    "effort_response_sequence = trunc_response_sequence.fillna(trunc_effort_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "effort_response_sequence.fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effort_response_sequence = effort_response_sequence.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save unique_events_dictionary to json\n",
    "with open('unique_events.json', 'w') as fp:\n",
    "    json.dump(unique_events_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save sequence of encountering events \n",
    "assistment_df.to_csv('Delft15_phase2/skill.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save effort_response sequence\n",
    "effort_response_sequence.to_csv(\"Delft15_phase2/effort_response.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_verticals.current_full_indices, my_verticals.current_full_indices_userids = my_verticals.create_full_indices_based_on_pre_index_data_ignoring_time_spent()\n",
    "# my_verticals.prepend_1_to_current_full_indices()\n",
    "# print(\"Len of full indices:\", len(my_verticals.current_full_indices))\n",
    "# print(\"Example sequence:\", my_verticals.current_full_indices[5])\n",
    "# sequence_max_len = 1500\n",
    "# X, y = my_verticals.expose_x_y(max_len = sequence_max_len)\n",
    "# y.shape\n",
    "# dummy = my_data.sorted_data\n",
    "# my_verticals.pre_index_data[my_verticals.pre_index_data.is_problem==1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
