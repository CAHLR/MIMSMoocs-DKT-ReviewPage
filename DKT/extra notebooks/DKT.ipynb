{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 1- delft 14'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"phase 1- delft 15\"\"\"\n",
    "\n",
    "data_dir='../code/data_dkt/delft15_phase1/'\n",
    "skill_dict=json.load(open('data_dkt/delft15_phase2/skill_dict_delft_all_0_336_all.json'))\n",
    "skill=os.path.join(data_dir+\"skill_df_delft_15_phase1.csv\")\n",
    "response=os.path.join(data_dir+\"response_df_delft_15_phase1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"phase 1- delft 14\"\"\"\n",
    "\n",
    "\n",
    "# data_dir='../code/data_dkt/delft14_phase1/'\n",
    "# skill_dict=json.load(open('data_dkt/delft15_phase2/skill_dict_delft_all_0_336_all.json'))\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_14_phase1.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_14_phase1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 2 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"phase 2 \"\"\"\n",
    "# data_dir = \"Delft15_phase2/\"\n",
    "# skill_dict=json.load(open(os.path.join(data_dir,\"unique_events.json\")))\n",
    "# response = os.path.join(data_dir,\"effort_response.csv\")\n",
    "# skill = os.path.join(data_dir,\"skill.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access ../code/data_dkt/delft15/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls ../code/data_dkt/delft15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data(skill_csv,response_csv,skill_json):\n",
    "    \"\"\"\n",
    "    This function reads data files and returns:\n",
    "    1. skill_dict:reads data from __skill_dict.json__ which maps skills to numbers 1,2..\n",
    "    2. skill_df: reads data from __skill.tsv__ which is a sequence of exercise or skills attempted by a student\n",
    "    3. response_df: reads data from correct.tsv which is sequence of binary response to skill/exercise in skill.tsv by each student\n",
    "    4. assistment_df: reads data from assistment_id.tsv which is a sequence of event id of the skill/exercises\n",
    "\n",
    "    \"\"\"\n",
    "    #response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # data_dir=\"data/dktData/\"\n",
    "    data_dir=\"\"\n",
    "    response_csv = os.path.join(data_dir,response_csv)\n",
    "\n",
    "    print(response_csv)\n",
    "\n",
    "    # response_df = pd.read_csv(response_csv, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    response_df = pd.read_csv(response_csv)\n",
    "    print(1)\n",
    "    skill_csv = os.path.join(data_dir,skill_csv)\n",
    "    # skill_df=pd.read_csv(skill_csv, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    skill_df=pd.read_csv(skill_csv)\n",
    "#     print(2)\n",
    "#     skill_dict=json.load(open(skill_json))\n",
    "#     print(3)\n",
    "\n",
    "    print(response_df.shape,skill_df.shape)\n",
    "    return response_df, skill_df, skill_dict\n",
    "\n",
    "\n",
    "def preprocess(skill_df, response_df, skill_num):\n",
    "    \"\"\"\n",
    "    This function extracts the skills and responses from the loaded files excluding student ids\n",
    "    :param skill_df: skills attempted by a student at each timestep\n",
    "    :param response_df: responses on the exercises  by a student at each timestep\n",
    "    :param skill_num: Total number of skills in the course\n",
    "    \"\"\"\n",
    "    skill_matrix = skill_df.iloc[:, 1:].values - 1 # 0 indexing skill numbers\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "    skill_array = one_hot(skill_matrix, skill_num)\n",
    "    skill_response_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    return skill_array, response_array, skill_response_array\n",
    "\n",
    "def one_hot(skill_matrix, vocab_size):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        skill_matrix: 2-D matrix (student, skills)\n",
    "        vocal_size: Number of skills in the course\n",
    "    returns:\n",
    "        a 3d-darray with a shape like (student, sequence_len, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "    seq_len = skill_matrix.shape[1] # sequence length is the number of the skills\n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    result = np.zeros((skill_matrix.shape[0], seq_len, vocab_size))\n",
    "    # iterate over each student in data\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in skills attempted by a student equal to 1\n",
    "        result[i, np.arange(seq_len), skill_matrix[i]] = 1.\n",
    "    return result\n",
    "\n",
    "def dkt_one_hot(skill_matrix, response_matrix, vocab_size):\n",
    "    \"\"\"\n",
    "   params:\n",
    "       skill_matrix: 2-D matrix (student, skills)\n",
    "       response_matrix:  2-D matrix (student, responses)\n",
    "       vocal_size: Number of skills in the course\n",
    "   returns:\n",
    "       a 3d-darray with a shape like (student, sequence_len, 2*vocab_size)\n",
    "   \"\"\"\n",
    "\n",
    "    seq_len = skill_matrix.shape[1]# sequence length is the number of the skills\n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    skill_response_array = np.zeros((skill_matrix.shape[0], seq_len, 2 * vocab_size))\n",
    "    # iterate over each student in data\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in correct response attempted by a student equal to 1\n",
    "        # 2* skill_matrix[i] goes to index in one hot encode for responses\n",
    "\n",
    "        skill_response_array[i, np.arange(seq_len), 2 * skill_matrix[i] + response_matrix[i]] = 1.\n",
    "    return skill_response_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Masking, Dense, Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import merge\n",
    "from keras.layers.merge import multiply\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "from theano import config\n",
    "from theano import printing\n",
    "from theano import function\n",
    "from keras.layers import Lambda\n",
    "import theano\n",
    "import numpy as np\n",
    "import pdb\n",
    "from math import sqrt\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "\n",
    "class DKTnet():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "        ## 1. First layer for the input (x_t), creates a tensor object\n",
    "        x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "        ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "        # the timestep will be masked (skipped) if all values in the input tensor\n",
    "        #  at that timestep are equal to mask_value\n",
    "        masked = Masking(mask_value=-1)(x)\n",
    "\n",
    "        ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "        # dimension as number of timesteps in input\n",
    "        lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "        ## 4. Add a fully connected layer on lstm layer\n",
    "        dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "        ## 5. Create  a tensor object --not sure if its required\n",
    "        y_order = Input(batch_shape=(None, None, self.input_dim_order), name='y_order')\n",
    "        merged = multiply([dense_out, y_order])\n",
    "\n",
    "        def reduce_dim(x):\n",
    "            #Custom tensor arithmatic from backend K\n",
    "            # chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "            x = K.max(x, axis=2, keepdims=True)\n",
    "            return x\n",
    "\n",
    "        def reduce_dim_shape(input_shape):\n",
    "            shape = list(input_shape)\n",
    "            shape[-1] = 1\n",
    "            return tuple(shape)\n",
    "\n",
    "        ## 6. Chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "        reduced = Lambda(reduce_dim, output_shape=reduce_dim_shape)(merged)\n",
    "\n",
    "        ## 7. Creates model object with specified input and output\n",
    "        self.model = Model(inputs=[x, y_order], outputs=reduced)\n",
    "\n",
    "        ## 8. Compile model by assigning loss function for backpropagtion\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        print('Summary of the model')\n",
    "        self.model.summary()\n",
    "\n",
    "    def train_on_batch(self, x_train, y_train, y_train_order):\n",
    "\n",
    "        self.model.train_on_batch([x_train, y_train_order], y_train)\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val, y_val_order):\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples.\n",
    "       :param x_train:\n",
    "       :param y_train:\n",
    "       :param y_train_order:\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "        print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch([x_val, y_val_order], y_val)\n",
    "\n",
    "    def predict(self, x_val, y_val_order):\n",
    "        return self.model.predict([x_val, y_val_order])\n",
    "\n",
    "    \n",
    "\n",
    "    def fit_data(self):\n",
    "        self.model.fit([self.x_train, self.y_train_order], \n",
    "                self.y_train, \n",
    "                batch_size=self.batch_size,\n",
    "                epochs=self.epochs,\n",
    "                callbacks=self.callbacks,\n",
    "                validation_split=self.validation_split, \n",
    "                validation_data=self.validation_data,\n",
    "                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import argparse\n",
    "\n",
    "# from DKT import DKTnet\n",
    "# from data_helper import load_data, one_hot, preprocess\n",
    "\n",
    "def get_callbacks():\n",
    "    '''\n",
    "    Some callback functions that you may find useful.\n",
    "    Please refer to https://keras.io/callbacks/ for more detailed explaination\n",
    "    '''\n",
    "    checkpoint = ModelCheckpoint('my_model', \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=2, \n",
    "                                 save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=2)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, min_lr=1e-4)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "def train(skill_array, response_array, skill_response_array):\n",
    "    '''\n",
    "    train the model with \n",
    "    '''\n",
    "    input_dim = skill_response_array.shape[-1] #2* num_skills\n",
    "    input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "\n",
    "    dkt = DKTnet(input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=skill_response_array[:, :-1, :], \n",
    "                y_train=response_array[:, :-1, np.newaxis], \n",
    "                y_train_order=skill_array[:, 1:, :],\n",
    "                validation_split=0.1,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=get_callbacks())\n",
    "    dkt.build()\n",
    "    dkt.fit_data()\n",
    "\n",
    "\n",
    "def batch_generator(skill_array, response_array, skill_response_array, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    return: batches of data from the original data set for training\n",
    "    \"\"\"\n",
    "    sample_num = skill_array.shape[0]\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(sample_num)\n",
    "        skill_array = skill_array.copy()[shuffled_indices]\n",
    "        response_array = response_array.copy()[shuffled_indices]\n",
    "        skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "        print('Training set shuffled')\n",
    "    for ndx in range(0, sample_num, batch_size):\n",
    "        skill_array_batch = skill_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        response_array_batch = response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        skill_response_array_batch = skill_response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        yield skill_response_array_batch, response_array_batch, skill_array_batch\n",
    "\n",
    "def create_validation_data(skill_array, response_array, skill_response_array,size=0.2):\n",
    "    \"\"\"\n",
    "    return: split of data from the original data set for testing\n",
    "    \"\"\"\n",
    "    sample_num = skill_array.shape[0]\n",
    "    shuffled_indices = np.random.permutation(sample_num)\n",
    "    skill_array = skill_array.copy()[shuffled_indices]\n",
    "    response_array = response_array.copy()[shuffled_indices]\n",
    "    skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "    print('Data set shuffled, preparing for split')\n",
    "    split_index=int(size*sample_num)\n",
    "    train_index=sample_num-split_index\n",
    "    print('train:', train_index, \"test:\",split_index)\n",
    "    skill_array_train = skill_array[0:train_index]\n",
    "    response_array_train = response_array[0:train_index]\n",
    "    skill_response_array_train = skill_response_array[0:train_index]\n",
    "    \n",
    "    skill_array_test = skill_array[train_index:]\n",
    "    response_array_test = response_array[train_index:]\n",
    "    skill_response_array_test = skill_response_array[train_index:]\n",
    "    \n",
    "    return skill_array_train, response_array_train, skill_response_array_train,skill_array_test,response_array_test,skill_response_array_test\n",
    "\n",
    "\n",
    "def train_on_batch(skill_array, response_array, skill_response_array):\n",
    "    \"\"\"This function creates a DKT MODEL object using DKT.py and\n",
    "    trains it using the batched data\"\"\"\n",
    "\n",
    "    input_dim = skill_response_array.shape[-1]  #2* num_skills\n",
    "    input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    print(\"batch size=\",batch_size)\n",
    "    print(\"epoch size=\", epochs)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    parameters like batch_size, epochs x_train, y_train, y_train_order and validations are useless\n",
    "    if you are doing a training by batch\n",
    "    '''\n",
    "    dkt = DKTnet(input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=skill_response_array[:, :-1, :], \n",
    "                y_train=response_array[:, :-1, np.newaxis], \n",
    "                y_train_order=skill_array[:, 1:, :],\n",
    "                validation_split=0.2,\n",
    "                validation_data=0.2,\n",
    "                optimizer='adam',\n",
    "                callbacks=None)\n",
    "\n",
    "    dkt.build_train_on_batch()\n",
    "\n",
    "    '''\n",
    "    For simplification, we are over fitting on the training set here.\n",
    "    In your model, you should do a train-test split or cross-validation which can be found in sklearn package.\n",
    "    '''\n",
    "    skill_array_train, response_array_train, skill_response_array_train,skill_array_test,response_array_test,skill_response_array_test=create_validation_data(skill_array, response_array, skill_response_array,size=dkt.validation_split)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('***Epoch', e+1, 'starts****')\n",
    "        iteration = 0\n",
    "        total_iteration_num = 1 + (skill_array.shape[0] - 1) // batch_size\n",
    "        for skill_response_array_batch, response_array_batch, skill_array_batch in batch_generator(skill_array_train, response_array_train, skill_response_array_train, batch_size=batch_size):\n",
    "            dkt.train_on_batch(skill_response_array_batch[:, :-1, :],response_array_batch[:, :-1, np.newaxis],  skill_array_batch[:, 1:, :])\n",
    "            iteration += 1\n",
    "        print(\"iter: {}/{} done\".format(iteration, total_iteration_num))\n",
    "        \n",
    "        result = dkt.test_on_batch(skill_response_array_test[:, :-1, :],response_array_test[:, :-1, np.newaxis],  skill_array_test[:, 1:, :])\n",
    "        trainresult = dkt.test_on_batch(skill_response_array_train[:, :-1, :],response_array_train[:, :-1, np.newaxis],  skill_array_train[:, 1:, :])\n",
    "        print('Evalutaion training data result', trainresult)\n",
    "        print('Evalutaion validation data result', result)\n",
    "        '''\n",
    "        You should implement your own evaluation function here to evaluate your result on the validation set if each sample have different timesteps\n",
    "        '''\n",
    "    prediction = dkt.predict(skill_response_array[0:1, :-1, :],\n",
    "                                skill_array[0:1, 1:, :])\n",
    "\n",
    "    print('Check Prediction Output:', prediction,response_array[:, :-1, np.newaxis])\n",
    "    # print('Check shape:Input,prediction, y_train:',dkt.x_train.shape, prediction.shape,dkt.y_train[0:1].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-S',\"--skill_df\", type=str, required=True)\n",
    "#     parser.add_argument('-R',\"--response_df\",type=str, required=True)\n",
    "#     parser.add_argument('-dict',\"--skill_dict\", type=str, required=True)\n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n",
    "\n",
    "#     response_df,skill_df,skill_dict=load_data(args.skill_df,args.response_df,args.skill_dict)\n",
    "#     # response_df,skill_df,skill_dict=load_data()\n",
    "#     skills_num = len(skill_dict)\n",
    "#     print('Number of skills are :{}'.format(skills_num))\n",
    "#     skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skills_num)\n",
    "#     # train(skill_array, response_array, skill_response_array)\n",
    "#     train_on_batch(skill_array, response_array, skill_response_array)\n",
    "\n",
    "#     # python train.py -S skill_df_delft_15.csv -R response_df_delft_15.csv -dict skill_dict_delft_15.json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../code/data_dkt/delft15_phase1/response_df_delft_15_phase1.csv\n",
      "1\n",
      "((9242, 101), (9242, 101))\n",
      "Number of skills are :336\n",
      "('skill_array, response_array, skill_response_array/n', (9242, 100, 336), (9242, 100), (9242, 100, 672))\n",
      "('batch size=', 64)\n",
      "('epoch size=', 10)\n",
      "DKTnet initialization done\n",
      "Summary of the model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "x (InputLayer)                   (None, None, 672)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_1 (Masking)              (None, None, 672)     0           x[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, None, 40)      114080      masking_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, None, 336)     13776       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "y_order (InputLayer)             (None, None, 336)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)            (None, None, 336)     0           dense_1[0][0]                    \n",
      "                                                                   y_order[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, None, 1)       0           multiply_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 127,856\n",
      "Trainable params: 127,856\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Data set shuffled, preparing for split\n",
      "('train:', 7394, 'test:', 1848)\n",
      "('***Epoch', 1, 'starts****')\n",
      "Training set shuffled\n",
      "iter: 116/145 done\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "response_df,skill_df,skill_dict=load_data(skill,response,skill_dict)\n",
    "skills_num = len(skill_dict)\n",
    "print('Number of skills are :{}'.format(skills_num))\n",
    "skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skills_num)\n",
    "print(\"skill_array, response_array, skill_response_array/n\",\n",
    "      skill_array.shape,response_array.shape,skill_response_array.shape)\n",
    "\n",
    "# print(skill_array[0])\n",
    "# print(response_array[0])\n",
    "# print(skill_response_array[0])\n",
    "\n",
    "# #train(skill_array, response_array, skill_response_array)\n",
    "train_on_batch(skill_array, response_array, skill_response_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
