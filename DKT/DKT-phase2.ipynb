{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 1- delft 14'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # \"\"\"phase 1- delft 15\"\"\"\n",
    "\n",
    "# data_dir=''\n",
    "# event_dict=('delft15_phase2/all_event_dict_delft15.json')\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_15.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"phase 1- delft 14\"\"\"\n",
    "\n",
    "\n",
    "# data_dir='../code/data_dkt/delft14_phase1/'\n",
    "# skill_dict=json.load(open('data_dkt/delft15_phase2/skill_dict_delft_all_0_336_all.json'))\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_14_phase1.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_14_phase1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 2 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"phase 2 \"\"\"\n",
    "# data_dir = \"Delft15_phase2/\"\n",
    "# skill_dict=json.load(open(os.path.join(data_dir,\"unique_events.json\")))\n",
    "# response = os.path.join(data_dir,\"effort_response.csv\")\n",
    "# skill = os.path.join(data_dir,\"skill.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ../code/data_dkt/delft15/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls ../code/data_dkt/delft15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed processing file data_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    " \n",
    "\"\"\" ONE HOT ENCODING OF SKILL AND RESPONSE DATA\"\"\"  \n",
    "\n",
    "\n",
    "def load_data(skill_csv,response_csv,skill_json,problem_num,is_behaviour=False):\n",
    "    \"\"\"\n",
    "    This function reads data files:\n",
    "    1. skill_dict:reads data from __skill_dict.json__ which maps skills to numbers 1,2..\n",
    "    2. skill_csv: reads data from __skill.csv__ which is a sequence of exercise or skills attempted by a student\n",
    "    3. response_csv: reads data from reponse.csv which is sequence of binary response to skill/exercise in skill.csv by each student\n",
    "    4. problem_num: total number of problem events in the course, each tagged from 1 to problem_num in skill_json which a dictionary that maps events to integers\n",
    "    5. is_behaviour is the boolean value which is TRUE when we want to include behavior events in our model or false for phase-1 dkt\n",
    "\n",
    "    \"\"\"\n",
    "    #response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # data_dir=\"data/dktData/\"\n",
    "    data_dir=\"\"\n",
    "    response_csv = os.path.join(data_dir,response_csv)\n",
    "\n",
    "    print(response_csv)\n",
    "\n",
    "    # response_df = pd.read_csv(response_csv, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    response_df = pd.read_csv(response_csv)\n",
    "    print(1)\n",
    "    skill_csv = os.path.join(data_dir,skill_csv)\n",
    "    # skill_df=pd.read_csv(skill_csv, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    skill_df=pd.read_csv(skill_csv)\n",
    "#     print(2)\n",
    "    skill_dict=json.load(open(skill_json))\n",
    "#     print(3)\n",
    "\n",
    "    print(response_df.shape,skill_df.shape)\n",
    "    if is_behaviour is False:\n",
    "        non_problem_event_num=0\n",
    "    else:\n",
    "        non_problem_event_num=len(skill_dict)-problem_num\n",
    "    return skill_df,response_df,problem_num, non_problem_event_num\n",
    "\n",
    "    \n",
    "def preprocess(skill_df, response_df, problem_num,non_problem_num):\n",
    "    \"\"\"\n",
    "    This function extracts the skills and responses from the loaded files excluding student ids\n",
    "    :param skill_df: skills attempted by a student at each timestep\n",
    "    :param response_df: responses on the exercises  by a student at each timestep\n",
    "    :param problem_num: Total number of problem events in the course\n",
    "    :param non_problem_num: Total number of non-problem events \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    #skill_matrix = skill_df.iloc[:, 1:].values - 1 # 0 indexing skill numbers\n",
    "    \n",
    "    skill_matrix = skill_df.iloc[:, 1:].values\n",
    "    \n",
    "#     1. skill array\n",
    "    skill_array =convert_event_to_one_hot(skill_matrix, problem_num+non_problem_num)\n",
    "    \n",
    "#     2. response array\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "    \n",
    "#     3. skill_response array\n",
    "    skill_response_array = append_response_one_hot(skill_array,skill_matrix, response_array,problem_num)\n",
    "    \n",
    "    print('skill_array.shape,response_array.shape,skill_response_array.shape')\n",
    "    print(skill_array.shape,response_array.shape,skill_response_array.shape)\n",
    "    \n",
    "    return skill_array, response_array, skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_event_to_one_hot(skill_matrix, vocab_size):\n",
    "    # 1. Create one hot encode row for each integer from 0 to vocab_size\n",
    "    print(vocab_size)\n",
    "    one_hot_dict = np.eye(vocab_size) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers, add last row with all zeros\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "#     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "    \n",
    "    \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    on_hot_sequence = np.empty((skill_matrix.shape[0],sequence_len, vocab_size))\n",
    "\n",
    "    for row in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in skills sequence of a student equal to 1, index of skills starts at 1\n",
    "        on_hot_sequence[row] = one_hot_dict [skill_matrix[row]-1] #grab 1-hot rows for integers in skill_matrix\n",
    "    print('Check encode in skill and one hot skill:', len(skill_matrix[skill_matrix!=0]),np.sum( on_hot_sequence))\n",
    "    return on_hot_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_response_one_hot(skill_array,skill_matrix, response_matrix, problem_num):\n",
    "    \"\"\"\n",
    "   params:\n",
    "       skill_matrix: 2-D matrix (student, skills)\n",
    "       response_matrix:  2-D matrix (student, responses)\n",
    "    with_response_vocab_size: Number of (2*problem events AND non-problem events) in the course\n",
    "   returns:\n",
    "       a 3d-darray with a shape like (student, sequence_len,problem_num)\n",
    "   \"\"\"\n",
    "    # 1. Create one hot encode row for each integer from 0 to problem_num\n",
    "    one_hot_dict_problems_only = np.eye(problem_num) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num)))\n",
    "    #     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num))) \n",
    "    # 3. Get problem  ids of problems that are correct so that they can be assigned 1 value in one hot encode\n",
    "#     all other  are 0, - ----this is why we encode are not encoding  any skill as  ----\n",
    "    skill_matrix_only_correct_problems=skill_matrix*response_matrix\n",
    "\n",
    "  \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    response_one_hot = np.empty((skill_matrix.shape[0],sequence_len, problem_num))\n",
    "    # iterate over each student in data\n",
    "    for i in range(response_matrix.shape[0]):\n",
    "        # set vocabulary values in skills attempted by a student equal to 1\n",
    "        # get encodes for sequences with non-zero problem\n",
    "        response_one_hot[i]=one_hot_dict_problems_only[skill_matrix_only_correct_problems[i]-1]\n",
    "        \n",
    "    print('Check number correct in response and one hot response:', np.sum(response_matrix),np.sum(response_one_hot))\n",
    "    skill_response_array=np.concatenate((skill_array,response_one_hot),axis=2)\n",
    "    print('skill_array.shape,response_one_hot.shape,skill_response_array.shape')\n",
    "    print(skill_array.shape,response_one_hot.shape,skill_response_array.shape)\n",
    "    return  skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Masking, Dense, Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import merge\n",
    "from keras.layers.merge import multiply\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "from theano import config\n",
    "from theano import printing\n",
    "from theano import function\n",
    "from keras.layers import Lambda\n",
    "import theano\n",
    "import numpy as np\n",
    "import pdb\n",
    "from math import sqrt\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "\n",
    "class DKTnet():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "        ## 1. First layer for the input (x_t), creates a tensor object\n",
    "        x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "        ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "        # the timestep will be masked (skipped) if all values in the input tensor\n",
    "        #  at that timestep are equal to mask_value\n",
    "        masked = Masking(mask_value=-1)(x)\n",
    "\n",
    "        ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "        # dimension as number of timesteps in input\n",
    "        lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "        ## 4. Add a fully connected layer on lstm layer\n",
    "        dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "        ## 5. Create  a tensor object --to get probbailities of events that correpond to a particular\n",
    "        # time step in loss\n",
    "        y_order = Input(batch_shape=(None, None, self.input_dim_order), name='y_order')\n",
    "#         elementwise multiply \n",
    "        merged = multiply([dense_out, y_order])\n",
    "\n",
    "        def reduce_dim(x):\n",
    "            #Custom tensor arithmatic from backend K\n",
    "            # chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "            x = K.max(x, axis=2, keepdims=True)\n",
    "            return x\n",
    "\n",
    "        def reduce_dim_shape(input_shape):\n",
    "            shape = list(input_shape)\n",
    "            shape[-1] = 1\n",
    "            return tuple(shape)\n",
    "\n",
    "        ## 6. Chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "        reduced = Lambda(reduce_dim, output_shape=reduce_dim_shape)(merged)\n",
    "\n",
    "        ## 7. Creates model object with specified input and output\n",
    "        self.model = Model(inputs=[x, y_order], outputs=reduced)\n",
    "\n",
    "        ## 8. Compile model by assigning loss function for backpropagtion\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        print('Summary of the model')\n",
    "        self.model.summary()\n",
    "\n",
    "    def train_on_batch(self, x_train, y_train, y_train_order):\n",
    "\n",
    "        self.model.train_on_batch([x_train, y_train_order], y_train)\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val, y_val_order):\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples.\n",
    "       :param x_train:\n",
    "       :param y_train:\n",
    "       :param y_train_order:\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "        print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch([x_val, y_val_order], y_val)\n",
    "\n",
    "    def predict(self, x_val, y_val_order):\n",
    "        return self.model.predict([x_val, y_val_order])\n",
    "\n",
    "    \n",
    "\n",
    "    def fit_data(self):\n",
    "        self.model.fit([self.x_train, self.y_train_order], \n",
    "                self.y_train, \n",
    "                batch_size=self.batch_size,\n",
    "                epochs=self.epochs,\n",
    "                callbacks=self.callbacks,\n",
    "                validation_split=self.validation_split, \n",
    "                validation_data=self.validation_data,\n",
    "                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import argparse\n",
    "\n",
    "# from DKT import DKTnet\n",
    "# from data_helper import load_data, one_hot, preprocess\n",
    "\n",
    "def get_callbacks():\n",
    "    '''\n",
    "    Some callback functions that you may find useful.\n",
    "    Please refer to https://keras.io/callbacks/ for more detailed explaination\n",
    "    '''\n",
    "    checkpoint = ModelCheckpoint('my_model', \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=2, \n",
    "                                 save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=2)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, min_lr=1e-4)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "def train(skill_array, response_array, skill_response_array):\n",
    "    '''\n",
    "    train the model with \n",
    "    '''\n",
    "    input_dim = skill_response_array.shape[-1] #2* num_skills\n",
    "    input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "\n",
    "    dkt = DKTnet(input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=skill_response_array[:, :-1, :], \n",
    "                y_train=response_array[:, :-1, np.newaxis], \n",
    "                y_train_order=skill_array[:, 1:, :],\n",
    "                validation_split=0.1,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=get_callbacks())\n",
    "    dkt.build()\n",
    "    dkt.fit_data()\n",
    "\n",
    "\n",
    "def batch_generator(skill_array, response_array, skill_response_array, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    return: batches of data from the original data set for training\n",
    "    \"\"\"\n",
    "    sample_num = skill_array.shape[0]\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(sample_num)\n",
    "        skill_array = skill_array.copy()[shuffled_indices]\n",
    "        response_array = response_array.copy()[shuffled_indices]\n",
    "        skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "        print('Training set shuffled')\n",
    "    for ndx in range(0, sample_num, batch_size):\n",
    "        skill_array_batch = skill_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        response_array_batch = response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        skill_response_array_batch = skill_response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        yield skill_response_array_batch, response_array_batch, skill_array_batch\n",
    "\n",
    "def create_validation_data(skill_array, response_array, skill_response_array,size=0.2):\n",
    "    \"\"\"\n",
    "    return: split of data from the original data set for testing\n",
    "    \"\"\"\n",
    "    sample_num = skill_array.shape[0]\n",
    "    shuffled_indices = np.random.permutation(sample_num)\n",
    "    skill_array = skill_array.copy()[shuffled_indices]\n",
    "    response_array = response_array.copy()[shuffled_indices]\n",
    "    skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "    print('Data set shuffled, preparing for split')\n",
    "    split_index=int(size*sample_num)\n",
    "    train_index=sample_num-split_index\n",
    "    print('train:', train_index, \"test:\",split_index)\n",
    "    skill_array_train = skill_array[0:train_index]\n",
    "    response_array_train = response_array[0:train_index]\n",
    "    skill_response_array_train = skill_response_array[0:train_index]\n",
    "    \n",
    "    skill_array_test = skill_array[train_index:]\n",
    "    response_array_test = response_array[train_index:]\n",
    "    skill_response_array_test = skill_response_array[train_index:]\n",
    "    \n",
    "    return skill_array_train, response_array_train, skill_response_array_train,skill_array_test,response_array_test,skill_response_array_test\n",
    "\n",
    "\n",
    "def train_on_batch(skill_array, response_array, skill_response_array):\n",
    "    \"\"\"This function creates a DKT MODEL object using DKT.py and\n",
    "    trains it using the batched data\"\"\"\n",
    "\n",
    "    input_dim = skill_response_array.shape[-1]  #2* num_skills\n",
    "    input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    print(\"batch size==\",batch_size)\n",
    "    print(\"epoch size=\", epochs)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    parameters like batch_size, epochs x_train, y_train, y_train_order and validations are useless\n",
    "    if you are doing a training by batch\n",
    "    '''\n",
    "    dkt = DKTnet(input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=skill_response_array[:, :-1, :], \n",
    "                y_train=response_array[:, :-1, np.newaxis], \n",
    "                y_train_order=skill_array[:, 1:, :],\n",
    "                validation_split=0.2,\n",
    "                validation_data=0.2,\n",
    "                optimizer='adam',\n",
    "                callbacks=None)\n",
    "\n",
    "    dkt.build_train_on_batch()\n",
    "\n",
    "    '''\n",
    "    For simplification, we are over fitting on the training set here.\n",
    "    In your model, you should do a train-test split or cross-validation which can be found in sklearn package.\n",
    "    '''\n",
    "    skill_array_train, response_array_train, skill_response_array_train,skill_array_test,response_array_test,skill_response_array_test=create_validation_data(skill_array, response_array, skill_response_array,size=dkt.validation_split)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('***Epoch', e+1, 'starts****')\n",
    "        iteration = 0\n",
    "        total_iteration_num = 1 + (skill_array.shape[0] - 1) // batch_size\n",
    "        for skill_response_array_batch, response_array_batch, skill_array_batch in batch_generator(skill_array_train, response_array_train, skill_response_array_train, batch_size=batch_size):\n",
    "            dkt.train_on_batch(skill_response_array_batch[:, :-1, :],response_array_batch[:, :-1, np.newaxis],  skill_array_batch[:, 1:, :])\n",
    "            iteration += 1\n",
    "        print(\"iter: {}/{} done\".format(iteration, total_iteration_num))\n",
    "        \n",
    "        result = dkt.test_on_batch(skill_response_array_test[:, :-1, :],response_array_test[:, :-1, np.newaxis],  skill_array_test[:, 1:, :])\n",
    "        trainresult = dkt.test_on_batch(skill_response_array_train[:, :-1, :],response_array_train[:, :-1, np.newaxis],  skill_array_train[:, 1:, :])\n",
    "        print('Evalutaion training data result', trainresult)\n",
    "        print('Evalutaion validation data result', result)\n",
    "        '''\n",
    "        You should implement your own evaluation function here to evaluate your result on the validation set if each sample have different timesteps\n",
    "        '''\n",
    "    prediction = dkt.predict(skill_response_array[0:1, :-1, :],\n",
    "                                skill_array[0:1, 1:, :])\n",
    "\n",
    "#     print('Check Prediction Output:', prediction,response_array[:, :-1, np.newaxis])\n",
    "    # print('Check shape:Input,prediction, y_train:',dkt.x_train.shape, prediction.shape,dkt.y_train[0:1].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-S',\"--skill_df\", type=str, required=True)\n",
    "#     parser.add_argument('-R',\"--response_df\",type=str, required=True)\n",
    "#     parser.add_argument('-dict',\"--skill_dict\", type=str, required=True)\n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n",
    "\n",
    "#     response_df,skill_df,skill_dict=load_data(args.skill_df,args.response_df,args.skill_dict)\n",
    "#     # response_df,skill_df,skill_dict=load_data()\n",
    "#     skills_num = len(skill_dict)\n",
    "#     print('Number of skills are :{}'.format(skills_num))\n",
    "#     skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skills_num)\n",
    "#     # train(skill_array, response_array, skill_response_array)\n",
    "#     train_on_batch(skill_array, response_array, skill_response_array)\n",
    "\n",
    "#     # python train.py -S skill_df_delft_15.csv -R response_df_delft_15.csv -dict skill_dict_delft_15.json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_data_new(skill_csv,response_csv,skill_json,problem_num, is_behaviour=False):\n",
    "    \"\"\"\n",
    "    This function reads data files and returns:\n",
    "    1. skill_dict:reads data from __skill_dict.json__ which maps skills to numbers 1,2..\n",
    "    2. skill_df: reads data from __skill.tsv__ which is a sequence of exercise or skills attempted by a student\n",
    "    3. response_df: reads data from correct.tsv which is sequence of binary response to skill/exercise in skill.tsv by each student\n",
    "    4. problem_num: total num of problem events in the course\n",
    "    \"\"\"\n",
    "   \n",
    "    response_df = pd.read_csv(response_csv)\n",
    "\n",
    "    skill_df=pd.read_csv(skill_csv)\n",
    "\n",
    "    event_dict=json.load(open(skill_json))\n",
    "    total_no_events=len(event_dict)\n",
    "\n",
    "\n",
    "    print('response_df.shape,skill_df.shape')\n",
    "    print(response_df.shape,skill_df.shape)\n",
    "    if is_behaviour is True:\n",
    "        non_problem_event_num=total_no_events-problem_num\n",
    "    else:\n",
    "        non_problem_event_num=0\n",
    "        \n",
    "    return skill_df,response_df,problem_num, non_problem_event_num\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_new(skill_df, response_df, problem_num,non_problem_num):\n",
    "    \"\"\"\n",
    "    This function extracts the skills and responses from the loaded files excluding student ids\n",
    "    :param skill_df: skills attempted by a student at each timestep\n",
    "    :param response_df: responses on the exercises  by a student at each timestep\n",
    "    :param problem_num: Total number of problem events in the course\n",
    "    :param non_problem_num: Total number of non-problem events \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    #skill_matrix = skill_df.iloc[:, 1:].values - 1 # 0 indexing skill numbers\n",
    "    \n",
    "    skill_matrix = skill_df.iloc[:, 1:].values\n",
    "    \n",
    "#     1. skill array\n",
    "    skill_array =convert_event_to_one_hot(skill_matrix, problem_num+non_problem_num)\n",
    "    \n",
    "#     2. response array\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "    \n",
    "#     3. skill_response array\n",
    "    skill_response_array = append_response_one_hot(skill_array,skill_matrix, response_array,problem_num)\n",
    "    \n",
    "    return skill_array, response_array, skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_event_to_one_hot(skill_matrix, vocab_size):\n",
    "    # 1. Create one hot encode row for each integer from 0 to vocab_size\n",
    "    one_hot_dict = np.eye(vocab_size) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size))) \n",
    "    \n",
    "    \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    on_hot_sequence = np.empty((skill_matrix.shape[0],sequence_len, vocab_size))\n",
    "\n",
    "    for row in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in skills sequence of a student equal to 1, index of skills starts at 1\n",
    "        on_hot_sequence[row] = one_hot_dict [skill_matrix[row]-1] #grab 1-hot rows for integers in skill_matrix\n",
    "    print('Check encode in skill and one hot skill:', len(skill_matrix[skill_matrix!=0]),np.sum( on_hot_sequence))\n",
    "    return on_hot_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_response_one_hot(skill_array,skill_matrix, response_matrix, problem_num):\n",
    "    \"\"\"\n",
    "   params:\n",
    "       skill_matrix: 2-D matrix (student, skills)\n",
    "       response_matrix:  2-D matrix (student, responses)\n",
    "    with_response_vocab_size: Number of (2*problem events AND non-problem events) in the course\n",
    "   returns:\n",
    "       a 3d-darray with a shape like (student, sequence_len,with_response_vocab_size)\n",
    "   \"\"\"\n",
    "    # 1. Create one hot encode row for each integer from 0 to problem_num\n",
    "    one_hot_dict_problems_only = np.eye(problem_num) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num))) \n",
    "    # 3. Get problem  ids of problems that are correct so that they can be assigned 1 value in one hot encode\n",
    "#     all other  are 0\n",
    "    skill_matrix_only_correct_problems=skill_matrix*response_matrix\n",
    "\n",
    "  \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    response_one_hot = np.empty((skill_matrix.shape[0],sequence_len, problem_num))\n",
    "    # iterate over each student in data\n",
    "    for i in range(response_matrix.shape[0]):\n",
    "        # set vocabulary values in skills attempted by a student equal to 1\n",
    "        # get encodes for sequences with non-zero problem\n",
    "        response_one_hot[i]=one_hot_dict_problems_only[skill_matrix_only_correct_problems[i]-1]\n",
    "        \n",
    "    print('Check number correct in response and one hot response:', np.sum(response_matrix),np.sum(response_one_hot))\n",
    "    skill_response_array=np.concatenate((skill_array,response_one_hot),axis=2)\n",
    "    print('skill_array.shape,response_one_hot.shape,skill_response_array.shape')\n",
    "    print(skill_array.shape,response_one_hot.shape,skill_response_array.shape)\n",
    "    return  skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_df_delft_15.csv\n",
      "1\n",
      "(9242, 101) (9242, 101)\n",
      "Number of non-problem events: 291\n",
      "Check encode in skill and one hot skill: 31801 31801.0\n",
      "Check number correct in response and one hot response: 14687 14682.0\n",
      "skill_array.shape,response_one_hot.shape,skill_response_array.shape\n",
      "(1000, 100, 627) (1000, 100, 336) (1000, 100, 963)\n",
      "skill_array.shape,response_array.shape,skill_response_array.shape\n",
      "(1000, 100, 627) (1000, 100) (1000, 100, 963)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \"\"\"phase 2- delft 15\"\"\"\n",
    "\n",
    "data_dir=''\n",
    "event_dict=('delft15_phase2/all_event_dict_delft15.json')\n",
    "skill=os.path.join(data_dir+\"skill_df_delft_15.csv\")\n",
    "response=os.path.join(data_dir+\"response_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "# subset for testing\n",
    "\n",
    "skill_df,response_df,problem_num, non_problem_event_num=load_data(skill,response,event_dict,problem_num=336,is_behaviour=True)\n",
    "skill_df=skill_df.iloc[:1000,:]\n",
    "response_df=response_df.iloc[:1000,:]\n",
    "print('Number of non-problem events:', non_problem_event_num)\n",
    "skill_array, response_array, skill_response_array=preprocess(skill_df,response_df,problem_num, non_problem_event_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size== 64\n",
      "epoch size= 5\n",
      "DKTnet initialization done\n",
      "Summary of the model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, None, 963)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 963)    0           x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 40)     160640      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 627)    25707       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "y_order (InputLayer)            (None, None, 627)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 627)    0           dense_1[0][0]                    \n",
      "                                                                 y_order[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1)      0           multiply_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 186,347\n",
      "Trainable params: 186,347\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Data set shuffled, preparing for split\n",
      "train: 800 test: 200\n",
      "***Epoch 1 starts****\n",
      "Training set shuffled\n",
      "iter: 13/16 done\n",
      "['loss', 'acc']\n",
      "['loss', 'acc']\n",
      "Evalutaion training data result [0.27598137, 0.90249997]\n",
      "Evalutaion validation data result [0.28785124, 0.89343435]\n",
      "***Epoch 2 starts****\n",
      "Training set shuffled\n",
      "iter: 13/16 done\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "train_on_batch(skill_array,response_array, skill_response_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
