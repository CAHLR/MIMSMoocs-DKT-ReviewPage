{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir2='code/delft15_phase2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_test = pd.read_csv(data_dir2+\"test_event_df_delft_15.csv\").iloc[:, 1:].values\n",
    "response_test = pd.read_csv(data_dir2+\"test_response_df_delft_15.csv\").iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_train = pd.read_csv(data_dir2+\"train_event_df_delft_15.csv\").iloc[:, 1:].values\n",
    "response_train = pd.read_csv(data_dir2+\"train_response_df_delft_15.csv\").iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(true_label, predicted_label):\n",
    "    return np.mean(np.equal(true_label, predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - 1 (Global mode) - Majority response entire problem set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the majority label in response_array_train \n",
    "# Create a new baseline prediction where all predicted values take the majority label \n",
    "# write an accuracy function\n",
    "def baseline_1(response_array_train, response_array_test):\n",
    "    total_elements = response_array_train.shape[0]* response_array_train.shape[1]\n",
    "    number_of_ones = response_array_train.sum()\n",
    "    number_of_zeros = total_elements - number_of_ones\n",
    "    if number_of_ones > number_of_zeros:\n",
    "        #majority_label = 1\n",
    "        baseline_prediction = np.ones([response_array_test.shape[0], response_array_test.shape[1]])\n",
    "    else:\n",
    "        #majority_label = 0\n",
    "        baseline_prediction = np.zeros([response_array_test.shape[0], response_array_test.shape[1]])\n",
    "    return baseline_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mode Baseline Accuracy:  0.84813\n"
     ]
    }
   ],
   "source": [
    "baseline_pred_1 = baseline_1(response_train, response_test)\n",
    "print(\"Global Mode Baseline Accuracy: \",find_accuracy(response_test, baseline_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - 2 (Mode question-wise) - Majority response per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_qn_dict(skill_array_train, response_array_train):\n",
    "    question_dict = {k:[0,0] for k in range(1,337)}\n",
    "    #This is dictionary that has problem id as key and values \n",
    "    #that represent number of zeroes (incorrect) and number of ones (correct) respectively\"\n",
    "    for i in range(skill_array_train.shape[0]):\n",
    "        for idx, prob_id in enumerate(skill_array_train[i]):\n",
    "            if prob_id >= 1 and prob_id <= 336:\n",
    "                if response_array_train[i][idx] == 0:\n",
    "                    question_dict[prob_id][0] = question_dict[prob_id][0] + 1\n",
    "                else: \n",
    "                    question_dict[prob_id][1] = question_dict[prob_id][1] + 1\n",
    "    return question_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_2(question_dict, response_array_test, skill_array_test):\n",
    "    num_student = response_array_test.shape[0]\n",
    "    num_steps = response_array_test.shape[1]\n",
    "    baseline_pred = np.zeros([num_student, num_steps])\n",
    "    for i in range(skill_array_test.shape[0]):\n",
    "        for idx, prob_id in enumerate(skill_array_test[i]): \n",
    "            if prob_id >= 1 and prob_id <= 336:\n",
    "                majority_label = np.argmax(question_dict[prob_id])\n",
    "                baseline_pred[i][idx]=majority_label\n",
    "    return baseline_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Question Mode Baseline Accuracy:  0.89832\n"
     ]
    }
   ],
   "source": [
    "question_dict = prepare_qn_dict(skill_train, response_train)\n",
    "baseline_pred_2 = baseline_2(question_dict, response_test, skill_test)\n",
    "print(\"Per Question Mode Baseline Accuracy: \",find_accuracy(response_test, baseline_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
