{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 1- delft 14'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # \"\"\"phase 1- delft 15\"\"\"\n",
    "\n",
    "# data_dir=''\n",
    "# event_dict=('delft15_phase2/all_event_dict_delft15.json')\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_15.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"phase 1- delft 14\"\"\"\n",
    "\n",
    "\n",
    "# data_dir='../code/data_dkt/delft14_phase1/'\n",
    "# skill_dict=json.load(open('data_dkt/delft15_phase2/skill_dict_delft_all_0_336_all.json'))\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_14_phase1.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_14_phase1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 2 '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"phase 2 \"\"\"\n",
    "# data_dir = \"Delft15_phase2/\"\n",
    "# skill_dict=json.load(open(os.path.join(data_dir,\"unique_events.json\")))\n",
    "# response = os.path.join(data_dir,\"effort_response.csv\")\n",
    "# skill = os.path.join(data_dir,\"skill.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access ../code/data_dkt/delft15/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls ../code/data_dkt/delft15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed processing file data_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    " \n",
    "\"\"\" ONE HOT ENCODING OF SKILL AND RESPONSE DATA\"\"\"  \n",
    "\n",
    "\n",
    "def load_data(skill_csv,response_csv,skill_json,problem_num,is_behaviour=False):\n",
    "    \"\"\"\n",
    "    This function reads data files:\n",
    "    1. skill_json:reads data from __skill_dict.json__ which maps skills to numbers 1,2..\n",
    "    2. skill_csv: reads data from __skill.csv__ which is a sequence of exercise or skills attempted by a student\n",
    "    3. response_csv: reads data from reponse.csv which is sequence of binary response to skill/exercise in skill.csv by each student\n",
    "    4. problem_num: total number of problem events in the course, each tagged from 1 to problem_num in skill_json which a dictionary that maps events to integers\n",
    "    5. is_behaviour is the boolean value which is TRUE when we want to include behavior events in our model\n",
    "    \"\"\"\n",
    "\n",
    "    response_df = pd.read_csv(response_csv)\n",
    "    skill_df=pd.read_csv(skill_csv)\n",
    "    skill_dict=json.load(open(skill_json))\n",
    "\n",
    "#   1. skill matrix, exclude username\n",
    "    skill_matrix = skill_df.iloc[:, 1:].values\n",
    "#   2. response matrix,exclude username\n",
    "    response_matrix = response_df.iloc[:, 1:].values\n",
    "    print('LOADED DATA:','skill_matrix .shape,response_matrix.shape')\n",
    "    print(skill_matrix.shape,response_matrix.shape)\n",
    "    if is_behaviour is False:\n",
    "        non_problem_event_num=0\n",
    "        skill_matrix[skill_matrix>problem_num]=0\n",
    "    else:\n",
    "        non_problem_event_num=len(skill_dict)-problem_num\n",
    "    return skill_matrix,response_matrix,problem_num, non_problem_event_num\n",
    "\n",
    "\n",
    "def preprocess(skill_matrix,response_matrix,problem_num=336, non_problem_event_num=291):\n",
    "    \"\"\"\n",
    "    This function extracts the skills and responses from the loaded files excluding student ids\n",
    "    :param skill_matrix: skills attempted by a student at each timestep\n",
    "    :param response_matrix: responses on the exercises  by a student at each timestep\n",
    "    :param problem_num: Total number of problem events in the course\n",
    "    :param non_problem_num: Total number of non-problem events \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#   print('CHECK:::NUMBER of NON- problem events, problem events :,', non_problem_event_num,problem_num)\n",
    "#   1. skillarray\n",
    "    skill_array =convert_event_to_one_hot(skill_matrix, problem_num+non_problem_event_num)\n",
    "      \n",
    "#  2. skill_response array\n",
    "    skill_response_array = append_response_one_hot(skill_array,skill_matrix, response_matrix,problem_num)\n",
    "    \n",
    "#     print('skill_array.shape,response_matrix.shape,skill_response_array.shape')\n",
    "#     print(skill_array.shape,response_matrix.shape,skill_response_array.shape)\n",
    "    \n",
    "    return skill_array, response_matrix, skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_event_to_one_hot(skill_matrix, vocab_size):\n",
    "    # 1. Create one hot encode row for each integer from 0 to vocab_size\n",
    "#     print(vocab_size)\n",
    "    one_hot_dict = np.eye(vocab_size) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers, add last row with all zeros\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "#     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    on_hot_sequence = np.empty((skill_matrix.shape[0],sequence_len, vocab_size))\n",
    "\n",
    "    for row in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in skills sequence of a student equal to 1, index of skills starts at 1\n",
    "        on_hot_sequence[row] = one_hot_dict [skill_matrix[row]-1] #grab 1-hot rows for integers in skill_matrix\n",
    "#     print('Check encode in skill and one hot skill:', len(skill_matrix[skill_matrix!=0]),np.sum( on_hot_sequence))\n",
    "    return on_hot_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_response_one_hot(skill_array,skill_matrix, response_matrix, problem_num):\n",
    "    \"\"\"\n",
    "   params:\n",
    "       skill_matrix: 2-D matrix (student, skills)\n",
    "       response_matrix:  2-D matrix (student, responses)\n",
    "    with_response_vocab_size: Number of (2*problem events AND non-problem events) in the course\n",
    "   returns:\n",
    "       a 3d-darray with a shape like (student, sequence_len,problem_num)\n",
    "   \"\"\"\n",
    "    # 1. Create one hot encode row for each integer from 0 to problem_num\n",
    "    one_hot_dict_problems_only = np.eye(problem_num) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num)))\n",
    "    #     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num))) \n",
    "    # 3. Get problem  ids of problems that are correct so that they can be assigned 1 value in one hot encode\n",
    "#     all other  are 0, - ----this is why we encode are not encoding  any skill as  ----\n",
    "    skill_matrix_only_correct_problems=skill_matrix*response_matrix\n",
    "\n",
    "  \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    response_one_hot = np.empty((skill_matrix.shape[0],sequence_len, problem_num))\n",
    "    # iterate over each student in data\n",
    "    for i in range(response_matrix.shape[0]):\n",
    "        # set vocabulary values in skills attempted by a student equal to 1\n",
    "        # get encodes for sequences with non-zero problem\n",
    "        response_one_hot[i]=one_hot_dict_problems_only[skill_matrix_only_correct_problems[i]-1]\n",
    "        \n",
    "#     print('Check number correct in response and one hot response:', np.sum(response_matrix),np.sum(response_one_hot))\n",
    "    skill_response_array=np.concatenate((skill_array,response_one_hot),axis=2)\n",
    "#     print('skill_array.shape,response_one_hot.shape,skill_response_array.shape')\n",
    "#     print(skill_array.shape,response_one_hot.shape,skill_response_array.shape)\n",
    "    return  skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Masking, Dense, Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import merge\n",
    "from keras.layers.merge import multiply\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "from theano import config\n",
    "from theano import printing\n",
    "from theano import function\n",
    "from keras.layers import Lambda\n",
    "import theano\n",
    "import numpy as np\n",
    "import pdb\n",
    "from math import sqrt\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "class DKTcustomloss():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def LossModel(y_train,dense_out):\n",
    "        dense_output= Input(batch_shape=(None, None, self.input_dim_order), name='predicted_prob')\n",
    "        y_order=Input(batch_shape=(None, None, self.input_dim_order), name='skill_array')\n",
    "   \n",
    "        merged = multiply([dense_out, y_order])\n",
    "         ## 6. Chooses the max value(which is a non zero number in merged) from the output of merged layer\n",
    "        # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "        reduced = Lambda(reduce_dim, output_shape=reduce_dim_shape)(merged)\n",
    "        cce = tf.losses.softmax_cross_entropy(self.y_train,reduced)\n",
    "        loss_m = Model(inputs=[dense_output,y_order], outputs=cce)\n",
    "        #it's important to make this model not trainable if it has weights \n",
    "        #(you should probably set these weights manually if that's the case)    \n",
    "        loss_m.trainable = False\n",
    "        return loss_m\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "            ## 1. First layer for the input (x_t), creates a tensor object\n",
    "            x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "            ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "            # the timestep will be masked (skipped) if all values in the input tensor\n",
    "            #  at that timestep are equal to mask_value\n",
    "            masked = Masking(mask_value=0)(x)\n",
    "\n",
    "            ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "            # dimension as number of timesteps in input\n",
    "            lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "            ## 4. Add a fully connected layer on lstm layer, \n",
    "            ### this gives us probabilities of all events at differnt timesteps\n",
    "            dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "            \n",
    "\n",
    "            def get_probability_of_timestep_event(x):\n",
    "                #Custom tensor arithmatic from backend K\n",
    "                # chooses the max value from the output of previous layer\n",
    "                # this will reduce dimension from one hot of skill_num to 1 for each timestep\n",
    "                x = K.max(x, axis=2, keepdims=True)\n",
    "                return x\n",
    "\n",
    "        \n",
    "             ## 5.1 Get problem event encoding ONLY: \n",
    "            #  ASSUMPTION:that skill dict has problem events only from keys 1:input_dim_order\n",
    "            \n",
    "\n",
    "            def get_skill_array(x):\n",
    "#                 get array for  1 to t+1\n",
    "                return x[:,1:,:self.input_dim_order]\n",
    "            \n",
    "            def get_probability_array(x):\n",
    "#               get prob array for  1 to t+1 which is 0 to t index of probability array\n",
    "                return x[:,:-1,:self.input_dim_order]\n",
    "            def reduce_timestep_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                shape[-1]=self.input_dim_order\n",
    "                return tuple(shape)\n",
    "\n",
    "            def custom_loss(y_train,dense_out):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                y_train=y_train[:,1:,:]\n",
    "\n",
    "            # gets one hot encode skill without response part 0-336\n",
    "                y_order=Lambda(get_skill_array)(x)\n",
    "            # get probs of skills at 1 to t timesteps from  predictions by model( it predicts from 1 to t+1) \n",
    "                probabilities_for_tplus1=Lambda( get_probability_array)(dense_out)\n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([probabilities_for_tplus1, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "\n",
    "                cce = K.mean(K.binary_crossentropy(y_train, reduced ))#, axis=-1)\n",
    "                return cce\n",
    "            \n",
    "            \n",
    "            def mask_prob(a):       \n",
    "                condition= tf.less_equal( a, tf.constant( 0.6 ) )  \n",
    "                # when con is true gets value from ist tensor if false then from 2nd tensor \n",
    "                s = tf.where (condition, tf.zeros_like(a), tf.ones_like(a)) \n",
    "                return s\n",
    "                             \n",
    "            def accuracy2(y_train,dense_out):\n",
    "                '''this function gets then 0 to t-1 probailities from dKT \n",
    "                and compares them with 1-t actual response\n",
    "                '''\n",
    "                y_train=y_train[:,1:,:]\n",
    "                y_order=Lambda(get_skill_array)(x) #get skill one hot encode of probs from \n",
    "                probabilities_for_tplus1=Lambda(get_probability_array)(dense_out)\n",
    "            \n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([probabilities_for_tplus1, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "             \n",
    "                label=Lambda(mask_prob)(reduced)\n",
    "\n",
    "#                 return K.mean(K.equal(y_train, K.round(reduced)))\n",
    "                return K.mean(K.equal(y_train, label))\n",
    "\n",
    "\n",
    "            def recall_(y_train,dense_out): \n",
    "                y_train=y_train[:,1:,:]\n",
    "                y_order=Lambda(get_skill_array)(x) #get skill one hot encode of probs from \n",
    "                probabilities_for_tplus1=Lambda(get_probability_array)(dense_out)\n",
    "            \n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([probabilities_for_tplus1, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "\n",
    "#                 predicted_label=K.round(reduced) #<0.5=0, >0.5=1\n",
    "                predicted_label=Lambda(mask_prob)(reduced)# <0.6 =0\n",
    "                predicted_true_positives = K.sum(mask_prob(y_train * reduced)) #tp\n",
    "    \n",
    "#                 predicted_true_positives = K.sum(K.round(K.clip(y_train * reduced, 0, 1))) #tp\n",
    "                possible_positives = K.sum(y_train)#tp+fn\n",
    "                recall = predicted_true_positives / (possible_positives + K.epsilon())\n",
    "#                 f1_score = 2*(recall * precision) / (recall + precision)              \n",
    "#                 print(\"METRICS: f1_score,recall,precision,accuracy\")\n",
    "                return recall\n",
    "      \n",
    "            def precision_(y_train,dense_out):\n",
    "                y_train=y_train[:,1:,:]\n",
    "                y_order=Lambda(get_skill_array)(x) #get skill one hot encode of probs from \n",
    "                probabilities_for_tplus1=Lambda(get_probability_array)(dense_out)\n",
    "            \n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([probabilities_for_tplus1, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "#                 predicted_label=K.round(reduced) #<0.5=0, >0.5=1\n",
    "                predicted_label=Lambda(mask_prob)(reduced)# <0.6 =0\n",
    "#                 predicted_true_positives = K.sum(K.round(K.clip(y_train * reduced, 0, 1))) #tp\n",
    "                predicted_true_positives = K.sum(mask_prob(y_train * reduced)) #tp\n",
    "                predicted_positives = K.sum(K.round(K.clip(reduced, 0, 1))) #tp+fp\n",
    "                precision = predicted_true_positives / (predicted_positives + K.epsilon())\n",
    "#                 f1_score = 2*(recall * precision) / (recall + precision)\n",
    "# #                 print(\"METRICS: f1_score,recall,precision,accuracy\")\n",
    "                return precision\n",
    "         \n",
    "\n",
    "            \n",
    "  \n",
    "\n",
    "            def accuracy_accumulated(y_train, dense_out):\n",
    "                                        \n",
    "                def get_cumulative_response_array(x):\n",
    "                    response= x[:,:,-self.input_dim_order:]\n",
    "                    for i in range(response.shape[1]):\n",
    "                        while i < response.shape[1]-1:\n",
    "                            response[:,i+1,:]=response[:,i+1,:]+response[:,i,:]\n",
    "                    return response\n",
    "                \n",
    "                \n",
    "   \n",
    "                def cal_accuracy(x,y):\n",
    "                    b= x==y\n",
    "                    corr= b.astype(int)\n",
    "                    return corr\n",
    "                cumulative_response=Lambda(get_cumulative_response_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "                \n",
    "                mask_predicted_prob = Lambda(mask_prob,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(dense_out)\n",
    "                \n",
    "                mmse=K.mean(K.abs(mask_predicted_prob-cumulative_response))\n",
    "                \n",
    "                return mmse\n",
    "                \n",
    "                \n",
    "            \n",
    "            def custom_metric_accuracy(y_train,dense_out):\n",
    "                '''\n",
    "                     \n",
    "                its gets the ratio of correct responses that got maximum accuracy in their respective timesteps\n",
    "                \n",
    "              \n",
    "                '''\n",
    "                print(\"using custom metric.....\")\n",
    "                def get_response_array(x):\n",
    "                    return x[:,:,-self.input_dim_order:]\n",
    "                \n",
    "                actual_response=Lambda(get_response_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "                print(actual_response.get_shape(),\"actual response\")\n",
    "                def idx_max_prob(x):\n",
    "                    max_idx= K.argmax(x,axis=-1)\n",
    "#                         convert to one hot, to tackle,incorrect question argmax\n",
    "                    one_hot_idx=tf.one_hot(max_idx,\n",
    "                                self.input_dim_order,\n",
    "                                on_value=1.0,\n",
    "                                off_value=0.0,\n",
    "                                axis=-1)\n",
    "                    return one_hot_idx\n",
    "        \n",
    "                max_predicted_prob = Lambda(idx_max_prob,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(dense_out)\n",
    "                print( max_predicted_prob.get_shape(),\" max_predicted_prob\")\n",
    "                # we use merging, we want to find how many 1-1 matchings are there\n",
    "                merged_metric = multiply([actual_response,  max_predicted_prob])\n",
    "\n",
    "\n",
    "                accuracy = K.mean(K.sum(merged_metric,axis=-1))\n",
    "                print(accuracy.get_shape())\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            ## 7.Creates model object with specified input and output\n",
    "                #CHANGED: OUTPUTS: TO GET probabilities at all timesteps as output.\n",
    "\n",
    "\n",
    "            def custom_metric_accuracy(y_train,dense_out):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                print(\"using custom metric.....\")\n",
    "                def get_response_array(x):\n",
    "                    return x[:,:,-self.input_dim_order:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                actual_response=Lambda(get_response_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "                print(actual_response.get_shape(),\"actual response\")\n",
    "                def idx_max_prob(x):\n",
    "                    max_idx= K.argmax(x,axis=-1)\n",
    "#                         convert to one hot, to tackle,incorrect question argmax\n",
    "                    one_hot_idx=tf.one_hot(max_idx,\n",
    "                                self.input_dim_order,\n",
    "                                on_value=1.0,\n",
    "                                off_value=0.0,\n",
    "                                axis=-1)\n",
    "                    return one_hot_idx\n",
    "        \n",
    "                max_predicted_prob = Lambda(idx_max_prob,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(dense_out)\n",
    "                print( max_predicted_prob.get_shape(),\" max_predicted_prob\")\n",
    "                # we use merging, we want to find how many 1-1 matchings are there\n",
    "                merged_metric = multiply([actual_response,  max_predicted_prob])\n",
    "\n",
    "\n",
    "                accuracy = K.mean(K.sum(merged_metric,axis=-1))\n",
    "                print(accuracy.get_shape())\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            ## 7.Creates model object with specified input and output\n",
    "                #CHANGED: OUTPUTS: TO GET probabilities at all timesteps as output.\n",
    "\n",
    "            self.model = Model(inputs=x, outputs=dense_out)\n",
    "\n",
    "            ## 8. Compile model by assigning loss function for backpropagtion\n",
    "            self.model.compile(optimizer=self.optimizer,\n",
    "                               loss=custom_loss,\n",
    "                               metrics=[recall_,precision_,accuracy2])\n",
    "\n",
    "            print('Summary of the model')\n",
    "            self.model.summary()\n",
    "   \n",
    "\n",
    "    def train_on_batch(self, x_train,y_train,y_train_order):\n",
    "# y_train_order: not used\n",
    "        self.model.train_on_batch(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val,y_train_order):\n",
    "        \n",
    "        # y_train_order: not used\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "#         print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch(x_val, y_val)\n",
    "\n",
    "    def predict(self, x_val,y_train_order):\n",
    "#              # y_train_order: not used\n",
    "        return self.model.predict_on_batch(x_val)\n",
    "\n",
    "    def save(self):\n",
    "             # Save the weights\n",
    "        print('saved model','phase1_model_weights.h5','model_architecture_phase1.json')\n",
    "        # Save the weights\n",
    "        self.model.save_weights('model_weights.h5')\n",
    "\n",
    "        # Save the model architecture\n",
    "        with open('model_architecture_phase1.json', 'w') as f:\n",
    "            f.write(self.model.to_json())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import argparse\n",
    "\n",
    "# from DKT import DKTnet\n",
    "# from data_helper import load_data, one_hot, preprocess\n",
    "\n",
    "def get_callbacks():\n",
    "    '''\n",
    "    Some callback functions that you may find useful.\n",
    "    Please refer to https://keras.io/callbacks/ for more detailed explaination\n",
    "    '''\n",
    "    checkpoint = ModelCheckpoint('my_model', \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=2, \n",
    "                                 save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=2)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, min_lr=1e-4)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "\n",
    "\n",
    "def batch_generator(skill_array, response_array, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    return: batches of data from the original data set for training\n",
    "    \n",
    "    \"\"\"\n",
    "    skill_array, response_array, skill_response_array=preprocess(skill_array, response_array)\n",
    "    sample_num = skill_array.shape[0]\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(sample_num)\n",
    "        skill_array = skill_array.copy()[shuffled_indices]\n",
    "        response_array = response_array.copy()[shuffled_indices]\n",
    "        skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "        print('Training set shuffled')\n",
    "    for ndx in range(0, sample_num, batch_size):\n",
    "        skill_array_batch = skill_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        response_array_batch = response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        skill_response_array_batch = skill_response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        yield skill_response_array_batch, response_array_batch, skill_array_batch\n",
    "\n",
    "def create_validation_data(skill_array, response_array,size=0.2):\n",
    "    \"\"\"\n",
    "    return: split of data from the original data set for testing\n",
    "    \"\"\"\n",
    "\n",
    "    sample_num = skill_array.shape[0]\n",
    "    shuffled_indices = np.random.permutation(sample_num)\n",
    "    skill_array = skill_array.copy()[shuffled_indices]\n",
    "    response_array = response_array.copy()[shuffled_indices]\n",
    "#     skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "    print('Data set shuffled, preparing for split')\n",
    "    split_index=int(size*sample_num)\n",
    "    train_index=sample_num-split_index\n",
    "    print('train:', train_index, \"test:\",split_index)\n",
    "    skill_array_train = skill_array[0:train_index]\n",
    "    response_array_train = response_array[0:train_index]\n",
    "#     skill_response_array_train = skill_response_array[0:train_index]\n",
    "    \n",
    "    skill_array_test = skill_array[train_index:]\n",
    "    response_array_test = response_array[train_index:]\n",
    "#     skill_response_array_test = skill_response_array[train_index:]\n",
    "    \n",
    "    return skill_array_train, response_array_train,skill_array_test,response_array_test\n",
    "\n",
    "\n",
    "def train_on_batch(skill_array, response_array,custom_loss=False,):\n",
    "    \"\"\"This function creates a DKT MODEL object using DKT.py and\n",
    "    trains it using the batched data\"\"\"\n",
    "\n",
    "    input_dim = 627+336  #2* num_skills\n",
    "#     input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    input_dim_order = 336\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    print(\"batch size==\",batch_size)\n",
    "    print(\"epoch size=\", epochs)\n",
    "    print(\"input_dim_order\",input_dim_order)\n",
    "    print(\"input_dim\",input_dim)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    parameters like batch_size, epochs x_train, y_train, y_train_order and validations are not used\n",
    "    if you are doing a training by batch\n",
    "    '''\n",
    "\n",
    "    dkt = DKTcustomloss(input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "#                     x_train=skill_response_array, \n",
    "#                     y_train=response_array[:, :, np.newaxis], \n",
    "#                     y_train_order=skill_array,\n",
    "                validation_split=0.2,\n",
    "                validation_data=0.2,\n",
    "                optimizer='adam',\n",
    "                callbacks=None)\n",
    "\n",
    "        \n",
    "        \n",
    "    print('x_train.shape,y_train.shape')\n",
    "    print(skill_array.shape,response_array.shape)\n",
    "   \n",
    "    dkt.build_train_on_batch()\n",
    "    \n",
    "\n",
    "    '''\n",
    "    For simplification, we are over fitting on the training set here.\n",
    "    In your model, you should do a train-test split or cross-validation which can be found in sklearn package.\n",
    "    '''\n",
    "    skill_array_train, response_array_train,skill_array_test,response_array_test=create_validation_data(skill_array, response_array,size=dkt.validation_split)\n",
    "\n",
    "    df_metrics=pd.DataFrame(columns=['epoch','train_loss','train_recall','train_precision','train_accuracy','test_loss','test_recall','test_precision','test_accuracy'])\n",
    "    for e in range(epochs):\n",
    "        print('***Epoch', e+1, 'starts****')\n",
    "        j = 0\n",
    "        train_loss=0\n",
    "        train_recall=0\n",
    "        train_precision=0\n",
    "        train_accuracy=0\n",
    "        test_loss=0\n",
    "        test_recall=0\n",
    "        test_precision=0\n",
    "        test_accuracy=0\n",
    "#         test_accuracy=np.zeros(3)\n",
    "        total_iteration_num = 1 + (skill_array.shape[0] - 1) // batch_size\n",
    "        for skill_response_array_batch, response_array_batch, skill_array_batch in batch_generator(skill_array_train, response_array_train,  batch_size=batch_size):\n",
    "            dkt.train_on_batch(skill_response_array_batch,response_array_batch[:, :, np.newaxis],  skill_array_batch)\n",
    "            loss,recall,precision,accuracy=dkt.test_on_batch(skill_response_array_batch,response_array_batch[:, :, np.newaxis],  skill_array_batch)\n",
    "            train_loss+=loss\n",
    "            train_recall+=recall\n",
    "            train_precision+=precision\n",
    "            train_accuracy+=accuracy\n",
    "            j += 1\n",
    "        train_metrics=[train_loss/j,train_recall/j,train_precision/j,train_accuracy/j]\n",
    "        print('Evalutaion training data result,loss,recall,precision,accuracy', train_metrics)\n",
    "        print(\"iter: {}/{} done\".format(j, total_iteration_num))\n",
    "        i=0\n",
    "        for sr, r, s in batch_generator(skill_array_test, response_array_test,  batch_size=batch_size):\n",
    "            loss,recall,precision,accuracy=dkt.test_on_batch(sr,r[:, :, np.newaxis], s)\n",
    "            test_loss+=loss\n",
    "            test_recall+=recall\n",
    "            test_precision+=precision\n",
    "            test_accuracy+=accuracy\n",
    "            i+=1\n",
    "        test_metrics=[test_loss/i,test_recall/i,test_precision/i,test_accuracy/i]\n",
    "        print('Evalutaion validation data result, loss, f1_score,recall,precision,accuracy',test_metrics)\n",
    "        all_metrics=[e]+train_metrics+test_metrics\n",
    "        df_metrics.loc[e]=all_metrics\n",
    "#     df_metrics.to_csv('phase_2_metrics.csv')\n",
    "\n",
    "    dkt.save()\n",
    "    print(df_metrics)\n",
    "    return dkt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED DATA: skill_matrix .shape,response_matrix.shape\n",
      "(8242, 100) (8242, 100)\n",
      "batch size== 64\n",
      "epoch size= 5\n",
      "input_dim_order 336\n",
      "input_dim 963\n",
      "DKTnet initialization done\n",
      "x_train.shape,y_train.shape\n",
      "(8242, 100) (8242, 100)\n",
      "Summary of the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, None, 963)         0         \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, None, 963)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 40)          160640    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 336)         13776     \n",
      "=================================================================\n",
      "Total params: 174,416\n",
      "Trainable params: 174,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Data set shuffled, preparing for split\n",
      "train: 6594 test: 1648\n",
      "***Epoch 1 starts****\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,recall,precision,accuracy [0.19551948465120333, 0.2652700519404159, 0.29154984448821497, 0.879351179760236]\n",
      "iter: 104/129 done\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, f1_score,recall,precision,accuracy [0.19223214857853377, 0.4301589521077963, 0.48469099746300626, 0.888682865179502]\n",
      "***Epoch 2 starts****\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,recall,precision,accuracy [0.17719044228299305, 0.4737393856048584, 0.5033446475863457, 0.9027246557749234]\n",
      "iter: 104/129 done\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, f1_score,recall,precision,accuracy [0.18252336119229978, 0.4440532418397757, 0.4867271677805827, 0.895339486690668]\n",
      "***Epoch 3 starts****\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,recall,precision,accuracy [0.1678733893741782, 0.5280645254712838, 0.5461923843966081, 0.9112900627347139]\n",
      "iter: 104/129 done\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, f1_score,recall,precision,accuracy [0.1713403113759481, 0.6075440736917349, 0.5887873700031867, 0.9118408858776093]\n",
      "***Epoch 4 starts****\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,recall,precision,accuracy [0.15779964396586785, 0.5820184842898295, 0.600490516768052, 0.9195319545956758]\n",
      "iter: 104/129 done\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, f1_score,recall,precision,accuracy [0.16420747568974128, 0.5724715556089695, 0.6156765153774848, 0.9140003873751714]\n",
      "***Epoch 5 starts****\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,recall,precision,accuracy [0.15255372183254132, 0.5977952769742563, 0.6159355925539365, 0.9228734425627269]\n",
      "iter: 104/129 done\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, f1_score,recall,precision,accuracy [0.16052391953193224, 0.5852505507377478, 0.6286828701312726, 0.9166058141451615]\n",
      "saved model phase1_model_weights.h5 model_architecture_phase1.json\n",
      "   epoch  train_loss  train_recall  train_precision  train_accuracy  \\\n",
      "0    0.0    0.195519      0.265270         0.291550        0.879351   \n",
      "1    1.0    0.177190      0.473739         0.503345        0.902725   \n",
      "2    2.0    0.167873      0.528065         0.546192        0.911290   \n",
      "3    3.0    0.157800      0.582018         0.600491        0.919532   \n",
      "4    4.0    0.152554      0.597795         0.615936        0.922873   \n",
      "\n",
      "   test_loss  test_recall  test_precision  test_accuracy  \n",
      "0   0.192232     0.430159        0.484691       0.888683  \n",
      "1   0.182523     0.444053        0.486727       0.895339  \n",
      "2   0.171340     0.607544        0.588787       0.911841  \n",
      "3   0.164207     0.572472        0.615677       0.914000  \n",
      "4   0.160524     0.585251        0.628683       0.916606  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \"\"\"phase 2- delft 15\"\"\"\n",
    "\n",
    "data_dir='code/delft15_phase2/'\n",
    "event_dict=(data_dir+'all_event_dict_delft15.json')\n",
    "skill=os.path.join(data_dir+\"train_event_df_delft_15.csv\")\n",
    "response=os.path.join(data_dir+\"train_response_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "# subset for testing\n",
    "\n",
    "skill_matrix,response_array,problem_num, non_problem_event_num=load_data(skill,response,event_dict,problem_num=336,is_behaviour=True)\n",
    "\n",
    "\n",
    "\n",
    "# skill_matrix=skill_matrix[:500,:]\n",
    "# response_array=response_array[:500,:]\n",
    "\n",
    "# # load preprocess in each batch, put in train_batch fynction\n",
    "# print('Number of non-problem events:', non_problem_event_num)\n",
    "# skill_array, response_array, skill_response_array=preprocess(skill_matrix,response_array,problem_num, non_problem_event_num)\n",
    "\n",
    "model=train_on_batch(skill_matrix,response_array,custom_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED DATA: skill_matrix .shape,response_matrix.shape\n",
      "(1000, 100) (1000, 100)\n",
      "Training set shuffled\n",
      "Evalutaion test data result, loss,recall,precision,accuracy [0.1525301605463028, 0.5937254548072814, 0.6363957464694977, 0.9219888210296631]\n"
     ]
    }
   ],
   "source": [
    "skill_test=os.path.join(data_dir+\"test_event_df_delft_15.csv\")\n",
    "response_test=os.path.join(data_dir+\"test_response_df_delft_15.csv\")\n",
    "# response_test = pd.read_csv(response)\n",
    "# skill_test=pd.read_csv(skill)\n",
    "\n",
    "# subset for testing\n",
    "skill_test,response_test,problem_num, non_problem_event_num=load_data(skill_test,response_test,event_dict,problem_num=336,is_behaviour=False)\n",
    "i=0\n",
    "test_loss=0\n",
    "test_recall=0\n",
    "test_precision=0\n",
    "test_accuracy=0\n",
    "for sr, r, s in batch_generator(skill_test, response_test,  batch_size=100):\n",
    "    loss,recall,precision,accuracy=model.test_on_batch(sr,r[:, :, np.newaxis], s)\n",
    "    test_loss+=loss\n",
    "    test_recall+=recall\n",
    "    test_precision+=precision\n",
    "    test_accuracy+=accuracy\n",
    "    i+=1\n",
    "test_metrics=[test_loss/i,test_recall/i,test_precision/i,test_accuracy/i]\n",
    "print('Evalutaion test data result, loss,recall,precision,accuracy',test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_batch(skill_array,response_array, skill_response_array,custom_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_batch(skill_array,response_array, skill_response_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[1,2]\n",
    "l=np.array(k)\n",
    "s=l+l\n",
    "s/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s=[]\n",
    "s=np.zeros(3)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[10, 15],\n",
       "        [ 6,  7]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=np.array([0,0,0,0,10,15,6,7]).reshape(2,2,2)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "s=(tf.argmax(k,axis=-1))\n",
    "s.eval().shape\n",
    "o=tf.one_hot(\n",
    "    s,\n",
    "    4,\n",
    "    on_value=3,\n",
    "    off_value=0,\n",
    "    axis=-1,\n",
    "    dtype=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "# s = tf.less_equal(c, tf.constant(2,2))\n",
    "# # s=tf.where(s,c,tf.constant(-1))\n",
    "\n",
    "\n",
    "\n",
    "a = tf.constant( [0.1,0.9,0.6,0.5] )       \n",
    "con= tf.less_equal( a, tf.constant( 0.6 ) )  \n",
    "# when con is true gets value from ist tensor if false then from 2nd tensor \n",
    "s = tf.where (k, tf.zeros_like(a), tf.ones_like(a)) \n",
    "\n",
    "s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes must be equal rank, but are 3 and 0 for 'Select_3' (op: 'Select') with input shapes: [2,2,4], [2,2,4], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 3 and 0 for 'Select_3' (op: 'Select') with input shapes: [2,2,4], [2,2,4], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6f857aaa593d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmask_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-6f857aaa593d>\u001b[0m in \u001b[0;36mmask_prob\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmask_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmask_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   2679\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2680\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2681\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2682\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   6627\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6628\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6629\u001b[0;31m         \"Select\", condition=condition, t=x, e=y, name=name)\n\u001b[0m\u001b[1;32m   6630\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6631\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3389\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1731\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1732\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1733\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes must be equal rank, but are 3 and 0 for 'Select_3' (op: 'Select') with input shapes: [2,2,4], [2,2,4], []."
     ]
    }
   ],
   "source": [
    "def mask_prob(x):\n",
    "    cond = tf.less_equal(x, tf.constant(3))\n",
    "    return tf.where(cond, x, tf.constant(-1))\n",
    "mask_prob(o).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean(K.sum(o,axis=-1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.sum(o,axis=-1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[0,0]\n",
    "s+=[1,2]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
