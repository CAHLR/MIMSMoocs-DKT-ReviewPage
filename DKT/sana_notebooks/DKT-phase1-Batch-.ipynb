{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 1- delft 14'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # \"\"\"phase 1- delft 15\"\"\"\n",
    "\n",
    "# data_dir=''\n",
    "# event_dict=('delft15_phase2/all_event_dict_delft15.json')\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_15.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"phase 1- delft 14\"\"\"\n",
    "\n",
    "\n",
    "# data_dir='../code/data_dkt/delft14_phase1/'\n",
    "# skill_dict=json.load(open('data_dkt/delft15_phase2/skill_dict_delft_all_0_336_all.json'))\n",
    "# skill=os.path.join(data_dir+\"skill_df_delft_14_phase1.csv\")\n",
    "# response=os.path.join(data_dir+\"response_df_delft_14_phase1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phase 2 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"phase 2 \"\"\"\n",
    "# data_dir = \"Delft15_phase2/\"\n",
    "# skill_dict=json.load(open(os.path.join(data_dir,\"unique_events.json\")))\n",
    "# response = os.path.join(data_dir,\"effort_response.csv\")\n",
    "# skill = os.path.join(data_dir,\"skill.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../code/data_dkt/delft15/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls ../code/data_dkt/delft15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed processing file data_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    " \n",
    "\"\"\" ONE HOT ENCODING OF SKILL AND RESPONSE DATA\"\"\"  \n",
    "\n",
    "\n",
    "def load_data(skill_csv,response_csv,skill_json,problem_num,is_behaviour=False):\n",
    "    \"\"\"\n",
    "    This function reads data files:\n",
    "    1. skill_dict:reads data from __skill_dict.json__ which maps skills to numbers 1,2..\n",
    "    2. skill_csv: reads data from __skill.csv__ which is a sequence of exercise or skills attempted by a student\n",
    "    3. response_csv: reads data from reponse.csv which is sequence of binary response to skill/exercise in skill.csv by each student\n",
    "    4. problem_num: total number of problem events in the course, each tagged from 1 to problem_num in skill_json which a dictionary that maps events to integers\n",
    "    5. is_behaviour is the boolean value which is TRUE when we want to include behavior events in our model or false for phase-1 dkt\n",
    "\n",
    "    \"\"\"\n",
    "    #response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    # data_dir=\"data/dktData/\"\n",
    "    data_dir=\"\"\n",
    "    response_csv = os.path.join(data_dir,response_csv)\n",
    "    # response_df = pd.read_csv(response_csv, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "    response_df = pd.read_csv(response_csv)\n",
    "    skill_csv = os.path.join(data_dir,skill_csv)\n",
    "    skill_df=pd.read_csv(skill_csv)\n",
    "    skill_dict=json.load(open(skill_json))\n",
    "\n",
    "\n",
    "     \n",
    "#   1. skill matrix\n",
    "    skill_matrix = skill_df.iloc[:, 1:].values\n",
    "#   2. response matrix\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "#     print('skill_matrix .shape,response_array.shape')\n",
    "#     print(skill_matrix .shape,response_array.shape)\n",
    "    if is_behaviour is False:\n",
    "        non_problem_event_num=0\n",
    "        skill_matrix[skill_matrix>problem_num]=0\n",
    "    else:\n",
    "        non_problem_event_num=len(skill_dict)-problem_num\n",
    "    return skill_matrix,response_array,problem_num, non_problem_event_num\n",
    "\n",
    "\n",
    "def preprocess(skill_matrix,response_array,problem_num=336, non_problem_event_num=0):\n",
    "    \"\"\"\n",
    "    This function extracts the skills and responses from the loaded files excluding student ids\n",
    "    :param skill_df: skills attempted by a student at each timestep\n",
    "    :param response_df: responses on the exercises  by a student at each timestep\n",
    "    :param problem_num: Total number of problem events in the course\n",
    "    :param non_problem_num: Total number of non-problem events \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#     print('CHECK:::NUMBER of NON- problem events, problem events :,', non_problem_event_num,problem_num)\n",
    "    \n",
    "    \n",
    "#   1. skillarray\n",
    "    skill_array =convert_event_to_one_hot(skill_matrix, problem_num+non_problem_event_num)\n",
    "      \n",
    "#  2. skill_response array\n",
    "    skill_response_array = append_response_one_hot(skill_array,skill_matrix, response_array,problem_num)\n",
    "    \n",
    "#     print('skill_array.shape,response_array.shape,skill_response_array.shape')\n",
    "    print(skill_array.shape,response_array.shape,skill_response_array.shape)\n",
    "    \n",
    "    return skill_array, response_array, skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_event_to_one_hot(skill_matrix, vocab_size):\n",
    "    # 1. Create one hot encode row for each integer from 0 to vocab_size\n",
    "    print(vocab_size)\n",
    "    one_hot_dict = np.eye(vocab_size) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers, add last row with all zeros\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "#     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict = np.vstack((one_hot_dict,np.zeros(vocab_size)))\n",
    "    \n",
    "    \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    on_hot_sequence = np.empty((skill_matrix.shape[0],sequence_len, vocab_size))\n",
    "\n",
    "    for row in range(skill_matrix.shape[0]):\n",
    "        # set vocabulary values in skills sequence of a student equal to 1, index of skills starts at 1\n",
    "        on_hot_sequence[row] = one_hot_dict [skill_matrix[row]-1] #grab 1-hot rows for integers in skill_matrix\n",
    "#     print('Check encode in skill and one hot skill:', len(skill_matrix[skill_matrix!=0]),np.sum( on_hot_sequence))\n",
    "    return on_hot_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_response_one_hot(skill_array,skill_matrix, response_matrix, problem_num):\n",
    "    \"\"\"\n",
    "   params:\n",
    "       skill_matrix: 2-D matrix (student, skills)\n",
    "       response_matrix:  2-D matrix (student, responses)\n",
    "    with_response_vocab_size: Number of (2*problem events AND non-problem events) in the course\n",
    "   returns:\n",
    "       a 3d-darray with a shape like (student, sequence_len,problem_num)\n",
    "   \"\"\"\n",
    "    # 1. Create one hot encode row for each integer from 0 to problem_num\n",
    "    one_hot_dict_problems_only = np.eye(problem_num) \n",
    "    # 2. TO ASSIGN [000] to all out of vocab_size numbers\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num)))\n",
    "    #     ADDED ONE MORE ROW SO THAT  IF WE FILL NA WITH -1 then -1-1= -2 ,\n",
    "# so 2nd last row becomes the assigned value\n",
    "    one_hot_dict_problems_only= np.vstack((one_hot_dict_problems_only,np.zeros(problem_num))) \n",
    "    # 3. Get problem  ids of problems that are correct so that they can be assigned 1 value in one hot encode\n",
    "#     all other  are 0, - ----this is why we encode are not encoding  any skill as  ----\n",
    "    skill_matrix_only_correct_problems=skill_matrix*response_matrix\n",
    "\n",
    "  \n",
    "    # sequence length is the number timesteps in the sequence\n",
    "    sequence_len = skill_matrix.shape[1] \n",
    "    # instantiation of a numpy array with all elements 0\n",
    "    response_one_hot = np.empty((skill_matrix.shape[0],sequence_len, problem_num))\n",
    "    # iterate over each student in data\n",
    "    for i in range(response_matrix.shape[0]):\n",
    "        # set vocabulary values in skills attempted by a student equal to 1\n",
    "        # get encodes for sequences with non-zero problem\n",
    "        response_one_hot[i]=one_hot_dict_problems_only[skill_matrix_only_correct_problems[i]-1]\n",
    "        \n",
    "#     print('Check number correct in response and one hot response:', np.sum(response_matrix),np.sum(response_one_hot))\n",
    "    skill_response_array=np.concatenate((skill_array,response_one_hot),axis=2)\n",
    "#     print('skill_array.shape,response_one_hot.shape,skill_response_array.shape')\n",
    "#     print(skill_array.shape,response_one_hot.shape,skill_response_array.shape)\n",
    "    return  skill_response_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Masking, Dense, Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import merge\n",
    "from keras.layers.merge import multiply\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "from theano import config\n",
    "from theano import printing\n",
    "from theano import function\n",
    "from keras.layers import Lambda\n",
    "import theano\n",
    "import numpy as np\n",
    "import pdb\n",
    "from math import sqrt\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "\n",
    "class DKTcustomloss():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def LossModel(y_train,dense_out):\n",
    "\n",
    "        dense_output= Input(batch_shape=(None, None, self.input_dim_order), name='predicted_prob')\n",
    "        y_order=Input(batch_shape=(None, None, self.input_dim_order), name='skill_array')\n",
    "   \n",
    "        merged = multiply([dense_out, y_order])\n",
    "         ## 6. Chooses the max value(which is a non zero number in merged) from the output of merged layer\n",
    "        # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "        reduced = Lambda(reduce_dim, output_shape=reduce_dim_shape)(merged)\n",
    "        cce = tf.losses.softmax_cross_entropy(self.y_train,reduced)\n",
    "        loss_m = Model(inputs=[dense_output,y_order], outputs=cce)\n",
    "        #it's important to make this model not trainable if it has weights \n",
    "        #(you should probably set these weights manually if that's the case)    \n",
    "        loss_m.trainable = False\n",
    "        return loss_m\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "            ## 1. First layer for the input (x_t), creates a tensor object\n",
    "            x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "            ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "            # the timestep will be masked (skipped) if all values in the input tensor\n",
    "            #  at that timestep are equal to mask_value\n",
    "            masked = Masking(mask_value=0)(x)\n",
    "\n",
    "            ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "            # dimension as number of timesteps in input\n",
    "            lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "            ## 4. Add a fully connected layer on lstm layer, \n",
    "            ### this gives us probabilities of all events at differnt timesteps\n",
    "            dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            def get_probability_of_timestep_event(x):\n",
    "                #Custom tensor arithmatic from backend K\n",
    "                # chooses the max value from the output of previous layer\n",
    "                # this will reduce dimension from one hot of skill_num to 1 for each timestep\n",
    "                x = K.max(x, axis=2, keepdims=True)\n",
    "                return x\n",
    "\n",
    "        \n",
    "             ## 5.1 Get problem event encoding ONLY: \n",
    "            #  ASSUMPTION:that skill dict has problem events only from keys 1:input_dim_order\n",
    "            \n",
    "            def get_skill_array(x):\n",
    "                return x[:,:,:self.input_dim_order]\n",
    "  \n",
    "            \n",
    "#             y_order=Lambda(get_skill_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "            \n",
    "#             ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "#             merged = multiply([dense_out, y_order])\n",
    "\n",
    "#             ## 6. Chooses the max value from the output of merged which is the prob\n",
    "#                 # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "#             reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "    \n",
    "#             ## 7. Creates model object with specified input and output\n",
    "#             self.model = Model(inputs=x, outputs=reduced)\n",
    "\n",
    "#             ## 8. Compile model by assigning loss function for backpropagtion\n",
    "#             self.model.compile(optimizer=self.optimizer,\n",
    "#                                loss='binary_crossentropy',\n",
    "#                                metrics=['accuracy'])\n",
    "\n",
    "#             print('Summary of the model')\n",
    "#             self.model.summary()\n",
    "\n",
    "\n",
    "            \n",
    "            def custom_loss(y_train,dense_out):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                print(\"using custom loss.....\")\n",
    "\n",
    "                \n",
    "                y_order=Lambda(get_skill_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "            \n",
    "\n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([dense_out, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "\n",
    "                cce = K.mean(K.binary_crossentropy(y_train, reduced ))#, axis=-1)\n",
    "                return cce\n",
    "            \n",
    "            def accuracy2(y_train,dense_out):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                print(\"using custom loss.....\")\n",
    "\n",
    "                \n",
    "                y_order=Lambda(get_skill_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "            \n",
    "\n",
    "                ## 5.2 In dense output only retain probability of event at that timestep all others 0\n",
    "                merged = multiply([dense_out, y_order])\n",
    "\n",
    "                ## 6. Chooses the max value from the output of merged which is the prob\n",
    "                    # this will reduce dimension from skill_num to 1 for each timestep\n",
    "\n",
    "                reduced = Lambda(get_probability_of_timestep_event , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "\n",
    "                return K.mean(K.equal(y_train, K.round(reduced)))\n",
    "\n",
    "\n",
    "            \n",
    "            def custom_metric_accuracy(y_train,dense_out):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                print(\"using custom metric.....\")\n",
    "                def get_response_array(x):\n",
    "                    return x[:,:,-self.input_dim_order:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                actual_response=Lambda(get_response_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "                print(actual_response.get_shape(),\"actual response\")\n",
    "                def idx_max_prob(x):\n",
    "                    max_idx= K.argmax(x,axis=-1)\n",
    "#                         convert to one hot, to tackle,incorrect question argmax\n",
    "                    one_hot_idx=tf.one_hot(max_idx,\n",
    "                                self.input_dim_order,\n",
    "                                on_value=1.0,\n",
    "                                off_value=0.0,\n",
    "                                axis=-1)\n",
    "                    return one_hot_idx\n",
    "        \n",
    "                max_predicted_prob = Lambda(idx_max_prob,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(dense_out)\n",
    "                print( max_predicted_prob.get_shape(),\" max_predicted_prob\")\n",
    "                # we use merging, we want to find how many 1-1 matchings are there\n",
    "                merged_metric = multiply([actual_response,  max_predicted_prob])\n",
    "\n",
    "\n",
    "                accuracy = K.mean(K.sum(merged_metric,axis=-1))\n",
    "                print(accuracy.get_shape())\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            ## 7.Creates model object with specified input and output\n",
    "                #CHANGED: OUTPUTS: TO GET probabilities at all timesteps as output.\n",
    "\n",
    "            self.model = Model(inputs=x, outputs=dense_out)\n",
    "\n",
    "            ## 8. Compile model by assigning loss function for backpropagtion\n",
    "            self.model.compile(optimizer=self.optimizer,\n",
    "                               loss=custom_loss,\n",
    "                               metrics=[accuracy2])\n",
    "\n",
    "            print('Summary of the model')\n",
    "            self.model.summary()\n",
    "   \n",
    "\n",
    "    def train_on_batch(self, x_train,y_train,y_train_order):\n",
    "# y_train_order: not used\n",
    "        self.model.train_on_batch(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val,y_train_order):\n",
    "        \n",
    "        # y_train_order: not used\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "#         print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch(x_val, y_val)\n",
    "\n",
    "    def predict(self, x_val,y_train_order):\n",
    "#              # y_train_order: not used\n",
    "        return self.model.predict_on_batch(x_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DKTnet():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim(2*NO SKILLS)\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "        ## 1. First layer for the input (x_t), creates a tensor object\n",
    "        x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "        ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "        # the timestep will be masked (skipped) if all values in the input tensor\n",
    "        #  at that timestep are equal to mask_value\n",
    "        masked = Masking(mask_value=-1)(x)\n",
    "\n",
    "        ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "        # dimension as number of timesteps in input\n",
    "        lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "        ## 4. Add a fully connected layer on lstm layer\n",
    "\n",
    "        dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "        ## 5. Create  a tensor object --not sure if its required\n",
    "        y_order = Input(batch_shape=(None, None, self.input_dim_order), name='y_order')\n",
    "        merged = multiply([dense_out, y_order])\n",
    "\n",
    "        def reduce_dim(x):\n",
    "            #Custom tensor arithmatic from backend K\n",
    "            # chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "            x = K.max(x, axis=2, keepdims=True)\n",
    "            return x\n",
    "\n",
    "        def reduce_dim_shape(input_shape):\n",
    "            shape = list(input_shape)\n",
    "            shape[-1] = 1\n",
    "            return tuple(shape)\n",
    "\n",
    "        ## 6. Chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "        reduced = Lambda(reduce_dim, output_shape=reduce_dim_shape)(merged)\n",
    "\n",
    "        ## 7. Creates model object with specified input and output\n",
    "        self.model = Model(inputs=[x, y_order], outputs=reduced)\n",
    "\n",
    "        ## 8. Compile model by assigning loss function for backpropagtion\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        print('Summary of the model')\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    def train_on_batch(self, x_train, y_train, y_train_order):\n",
    "\n",
    "        self.model.train_on_batch([x_train, y_train_order], y_train)\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val, y_val_order):\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples.\n",
    "       :param x_train:\n",
    "       :param y_train:\n",
    "       :param y_train_order:\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "        print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch([x_val, y_val_order], y_val)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x_val, y_val_order):\n",
    "        return self.model.predict([x_val, y_val_order])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing :\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Masking, Dense, Embedding\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "class DKTnettest():\n",
    "\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                input_dim_order, \n",
    "                hidden_layer_size, \n",
    "                batch_size, \n",
    "                epochs,\n",
    "                x_train=[], \n",
    "                y_train=[], \n",
    "                y_train_order=[],\n",
    "                validation_split=0.0,\n",
    "                validation_data=None,\n",
    "                optimizer='adam',\n",
    "                callbacks=None,\n",
    "                problem_no=336):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dim: dimension of the input at one timestamp (dimension of x_t= 2*num_skills)\n",
    "        :param input_dim_order: dimension of the one-hot representation of problem to check order of occurence(=num_skills)\n",
    "        :param hidden_layer_size: number of nodes in hidden layer\n",
    "        :param x_train: 3D matrix of size (samples, number of timestamp/sequence length, dimension of input vec (x_t) )\n",
    "        :param y_train: a matrix of responses (samples,number of timestamp/sequence length)\n",
    "        :param y_train_order: shape of output equal to number of timesteps\n",
    "        :param validation_split:\n",
    "        :param validation_data:\n",
    "        :param optimizer:\n",
    "        :param callbacks:\n",
    "\n",
    "        \"\"\"\n",
    "        ## input dim is the dimension of the input at one timestamp (dimension of x_t)\n",
    "        self.input_dim = int(input_dim) #2* num_skills\n",
    "\n",
    "        ## input_dim_order is the dimension of the one-hot representation of problem\n",
    "        ## CHECK: order of occurence of responses should be according to timestamp\n",
    "        self.input_dim_order = int(input_dim_order)#num_skills\n",
    "\n",
    "        self.hidden_layer_size = int(hidden_layer_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "        ## x_train is a 3D matrix of size (samples, number of timestamp, dimension of input vec (x_t) )\n",
    "        ## in cognitive tutor # of students * # total responses * # input_dim(2*NO SKILLS)\n",
    "        self.x_train = x_train\n",
    "        ## y_train is a matrix of (samples one hot representation according to problem output value at each timestamp (y_t) )\n",
    "        self.y_train = y_train\n",
    "        ## y_train_order is the one hot representation of problem according to timestamp starting from\n",
    "        ## t=1 if training starts at t=0\n",
    "        self.y_train_order = y_train_order\n",
    "        # users: no of student datapoints\n",
    "        self.users = np.shape(x_train)[0]\n",
    "        self.validation_split = validation_split\n",
    "        self.validation_data = validation_data\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks\n",
    "        print (\"DKTnet initialization done\")\n",
    "\n",
    "    def build_train_on_batch(self):\n",
    "        ## 1. First layer for the input (x_t), creates a tensor object\n",
    "        x = Input(batch_shape=(None, None, self.input_dim), name='x')\n",
    "\n",
    "        ## 2. Mask unknown or anomalous valued timesteps in x\n",
    "        # the timestep will be masked (skipped) if all values in the input tensor\n",
    "        #  at that timestep are equal to mask_value\n",
    "        masked = Masking(mask_value=0)(x)\n",
    "\n",
    "        ## 3. Add a lstm layer, return sequences is True to allow output have same\n",
    "        # dimension as number of timesteps in input\n",
    "        lstm_out = LSTM(self.hidden_layer_size, return_sequences=True)(masked)\n",
    "\n",
    "        ## 4. Add a fully connected layer on lstm layer\n",
    "\n",
    "        dense_out = Dense(self.input_dim_order, activation='sigmoid')(lstm_out)\n",
    "       \n",
    "      \n",
    "\n",
    "        def reduce_dim(x):\n",
    "            #Custom tensor arithmatic from backend K\n",
    "            # chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "            x = K.max(x, axis=2, keepdims=True)\n",
    "            return x\n",
    "\n",
    "        def get_skill_array(x):\n",
    "            return x[:,:,:self.input_dim_order]\n",
    "        \n",
    "        \n",
    "        \n",
    "         ## 5. Create  a tensor object --not sure if its required\n",
    "#         y_order = Input(batch_shape=(None, None, self.input_dim_order), name='y_order')\n",
    "\n",
    "#         y_order=x[:,:,:self.input_dim_order]\n",
    "        y_order=Lambda(get_skill_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "\n",
    "        merged = multiply([dense_out, y_order])\n",
    "\n",
    "        ## 6. Chooses the max value from the output of previous layer\n",
    "            # this will reduce dimension from skill_num to 1 for each timestep\n",
    "#         reduced = Lambda(reduce_dim )(merged)\n",
    "        reduced = Lambda(reduce_dim , output_shape=lambda s: (s[0], s[1],1))(merged)\n",
    "        print(\"DIM OF y_order,reduced \",y_order.get_shape(),reduced.get_shape())\n",
    "        \n",
    "        \n",
    "        \n",
    "        def custom_metric_accuracy(u,v):\n",
    "                '''\n",
    "                using crossentropy, choose from dense_out, the probabilities corresponding to event attemted\n",
    "                at that timesetep. Create a tensor object of that accepts skill_array i. e one hot encoded skill sequence\n",
    "                then mask the dense_out using that skill array\n",
    "                '''\n",
    "                print(\"using custom metric.....\")\n",
    "                def get_response_array(x):\n",
    "                    return x[:,:,-self.input_dim_order:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                actual_response=Lambda(get_response_array,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(x)\n",
    "                print(actual_response.get_shape(),\"actual response\")\n",
    "                def idx_max_prob(x):\n",
    "                    max_idx= K.argmax(x,axis=-1)\n",
    "#                         convert to one hot, to tackle,incorrect question argmax\n",
    "                    one_hot_idx=tf.one_hot(max_idx,\n",
    "                                self.input_dim_order,\n",
    "                                on_value=1.0,\n",
    "                                off_value=0.0,\n",
    "                                axis=-1)\n",
    "                    return one_hot_idx\n",
    "        \n",
    "                max_predicted_prob = Lambda(idx_max_prob,output_shape=lambda s: (s[0], s[1],self.input_dim_order))(dense_out)\n",
    "                print( max_predicted_prob.get_shape(),\" max_predicted_prob\")\n",
    "                # we use merging, we want to find how many 1-1 matchings are there\n",
    "                merged_metric = multiply([actual_response,  max_predicted_prob])\n",
    "\n",
    "\n",
    "                accuracy = K.mean(K.sum(merged_metric,axis=-1))\n",
    "                print(accuracy.get_shape())\n",
    "                return accuracy\n",
    "\n",
    "        ## 7. Creates model object with specified input and output\n",
    "        self.model = Model(inputs=x, outputs=reduced)\n",
    "\n",
    "        ## 8. Compile model by assigning loss function for backpropagtion\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy',custom_metric_accuracy])\n",
    "\n",
    "        print('Summary of the model')\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train_on_batch(self, x_train,y_train,y_train_order):\n",
    "# y_train_order: not used\n",
    "        self.model.train_on_batch(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_on_batch(self, x_val, y_val,y_train_order):\n",
    "        \n",
    "        # y_train_order: not used\n",
    "        \"\"\"\n",
    "       Test the model on a single batch of samples\n",
    "       :return: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics).\n",
    "       The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "       \"\"\"\n",
    "#         print(self.model.metrics_names)\n",
    "        return self.model.test_on_batch(x_val, y_val)\n",
    "\n",
    "    def predict(self, x_val,y_train_order):\n",
    "#              # y_train_order: not used\n",
    "        return self.model.predict_on_batch(x_val)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import argparse\n",
    "\n",
    "# from DKT import DKTnet\n",
    "# from data_helper import load_data, one_hot, preprocess\n",
    "\n",
    "def get_callbacks():\n",
    "    '''\n",
    "    Some callback functions that you may find useful.\n",
    "    Please refer to https://keras.io/callbacks/ for more detailed explaination\n",
    "    '''\n",
    "    checkpoint = ModelCheckpoint('my_model', \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=2, \n",
    "                                 save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=2)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, min_lr=1e-4)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "\n",
    "\n",
    "def batch_generator(skill_array, response_array, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    return: batches of data from the original data set for training\n",
    "    \n",
    "    \"\"\"\n",
    "    skill_array, response_array, skill_response_array=preprocess(skill_array, response_array)\n",
    "    sample_num = skill_array.shape[0]\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(sample_num)\n",
    "        skill_array = skill_array.copy()[shuffled_indices]\n",
    "        response_array = response_array.copy()[shuffled_indices]\n",
    "        skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "        print('Training set shuffled')\n",
    "    for ndx in range(0, sample_num, batch_size):\n",
    "        skill_array_batch = skill_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        response_array_batch = response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        skill_response_array_batch = skill_response_array[ndx:min(ndx+batch_size, sample_num)]\n",
    "        yield skill_response_array_batch, response_array_batch, skill_array_batch\n",
    "\n",
    "def create_validation_data(skill_array, response_array,size=0.2):\n",
    "    \"\"\"\n",
    "    return: split of data from the original data set for testing\n",
    "    \"\"\"\n",
    "\n",
    "    sample_num = skill_array.shape[0]\n",
    "    shuffled_indices = np.random.permutation(sample_num)\n",
    "    skill_array = skill_array.copy()[shuffled_indices]\n",
    "    response_array = response_array.copy()[shuffled_indices]\n",
    "#     skill_response_array = skill_response_array.copy()[shuffled_indices]\n",
    "    print('Data set shuffled, preparing for split')\n",
    "    split_index=int(size*sample_num)\n",
    "    train_index=sample_num-split_index\n",
    "    print('train:', train_index, \"test:\",split_index)\n",
    "    skill_array_train = skill_array[0:train_index]\n",
    "    response_array_train = response_array[0:train_index]\n",
    "#     skill_response_array_train = skill_response_array[0:train_index]\n",
    "    \n",
    "    skill_array_test = skill_array[train_index:]\n",
    "    response_array_test = response_array[train_index:]\n",
    "#     skill_response_array_test = skill_response_array[train_index:]\n",
    "    \n",
    "    return skill_array_train, response_array_train,skill_array_test,response_array_test\n",
    "\n",
    "\n",
    "def train_on_batch(skill_array, response_array,custom_loss=False,):\n",
    "    \"\"\"This function creates a DKT MODEL object using DKT.py and\n",
    "    trains it using the batched data\"\"\"\n",
    "\n",
    "    input_dim = 2*336  #2* num_skills\n",
    "#     input_dim_order = skill_array.shape[-1] #num_skills\n",
    "    input_dim_order = 336\n",
    "    hidden_layer_size = 40\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    print(\"batch size==\",batch_size)\n",
    "    print(\"epoch size=\", epochs)\n",
    "    print(\"input_dim_order\",input_dim_order)\n",
    "    print(\"input_dim\",input_dim)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    parameters like batch_size, epochs x_train, y_train, y_train_order and validations are useless\n",
    "    if you are doing a training by batch\n",
    "    '''\n",
    "    if custom_loss ==False:\n",
    "        dkt = DKTnettest(input_dim, \n",
    "                    input_dim_order, \n",
    "                    hidden_layer_size, \n",
    "                    batch_size, \n",
    "                    epochs,\n",
    "#                     x_train=skill_response_array[:, :, :], \n",
    "#                     y_train=response_array[:, :, np.newaxis], \n",
    "#                     y_train_order=skill_array[:, :, :],\n",
    "                    validation_split=0.2,\n",
    "                    validation_data=0.2,\n",
    "                    optimizer='adam',\n",
    "                    callbacks=None)\n",
    "        \n",
    "    else:\n",
    "        dkt = DKTcustomloss(input_dim, \n",
    "                    input_dim_order, \n",
    "                    hidden_layer_size, \n",
    "                    batch_size, \n",
    "                    epochs,\n",
    "#                     x_train=skill_response_array[:, :, :], \n",
    "#                     y_train=response_array[:, :, np.newaxis], \n",
    "#                     y_train_order=skill_array[:, :, :],\n",
    "                    validation_split=0.2,\n",
    "                    validation_data=0.2,\n",
    "                    optimizer='adam',\n",
    "                    callbacks=None)\n",
    "        \n",
    "        \n",
    "    print('x_train.shape,y_train.shape')\n",
    "   \n",
    "    print(skill_array.shape,response_array.shape)\n",
    "   \n",
    "    dkt.build_train_on_batch()\n",
    "    \n",
    "\n",
    "    '''\n",
    "    For simplification, we are over fitting on the training set here.\n",
    "    In your model, you should do a train-test split or cross-validation which can be found in sklearn package.\n",
    "    '''\n",
    "    skill_array_train, response_array_train,skill_array_test,response_array_test=create_validation_data(skill_array, response_array,size=dkt.validation_split)\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('***Epoch', e+1, 'starts****')\n",
    "        iteration = 0\n",
    "        train_loss=0\n",
    "        train_accuracy=0\n",
    "        test_loss=0\n",
    "        test_accuracy=0\n",
    "        total_iteration_num = 1 + (skill_array.shape[0] - 1) // batch_size\n",
    "        for skill_response_array_batch, response_array_batch, skill_array_batch in batch_generator(skill_array_train, response_array_train,  batch_size=batch_size):\n",
    "            dkt.train_on_batch(skill_response_array_batch[:, :, :],response_array_batch[:, :, np.newaxis],  skill_array_batch[:, :, :])\n",
    "            loss,acc=dkt.test_on_batch(skill_response_array_batch[:, :, :],response_array_batch[:, :, np.newaxis],  skill_array_batch[:, :, :])\n",
    "            train_loss+=loss\n",
    "            train_accuracy+=acc\n",
    "            iteration += 1\n",
    "        print('Evalutaion training data result,loss,accuracy', train_loss/iteration,train_accuracy/iteration)\n",
    "        print(\"iter: {}/{} done\".format(iteration, total_iteration_num))\n",
    "        train_result=dkt.test_on_batch(skill_response_array_batch[:, :, :],response_array_batch[:, :, np.newaxis],  skill_array_batch[:, :, :])[0]\n",
    "        test_batch_i=0\n",
    "        for sr, r, s in batch_generator(skill_array_test, response_array_test,  batch_size=batch_size):\n",
    "           \n",
    "            l,a=dkt.test_on_batch(sr,r[:, :, np.newaxis], s)\n",
    "            test_loss+=l\n",
    "            test_accuracy+=a\n",
    "            test_batch_i+=1\n",
    "\n",
    "#         skill_array_test,response_array_test, skill_response_array_test=preprocess(skill_array_test,response_array_test)\n",
    "#         result = dkt.test_on_batch(skill_response_array_test[:, :, :],response_array_test[:, :, np.newaxis],  skill_array_test[:, :, :])\n",
    "#         print('Evalutaion training data result', train_result)\n",
    "        print('Evalutaion validation data result, loss, accuracy', test_loss/test_batch_i,test_accuracy/test_batch_i)\n",
    "        '''\n",
    "        You should implement your own evaluation function here to evaluate your result on the validation set if each sample have different timesteps\n",
    "        '''\n",
    "#     prediction = dkt.predict(skill_response_test[0:1, :, :],\n",
    "#                                 skill_array[0:1, 1:, :])\n",
    "\n",
    "#     print('Check Prediction Output:', prediction,response_array[:, :, np.newaxis])\n",
    "    # print('Check shape:Input,prediction, y_train:',dkt.x_train.shape, prediction.shape,dkt.y_train[0:1].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-S',\"--skill_df\", type=str, required=True)\n",
    "#     parser.add_argument('-R',\"--response_df\",type=str, required=True)\n",
    "#     parser.add_argument('-dict',\"--skill_dict\", type=str, required=True)\n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n",
    "\n",
    "#     response_df,skill_df,skill_dict=load_data(args.skill_df,args.response_df,args.skill_dict)\n",
    "#     # response_df,skill_df,skill_dict=load_data()\n",
    "#     skills_num = len(skill_dict)\n",
    "#     print('Number of skills are :{}'.format(skills_num))\n",
    "#     skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skills_num)\n",
    "#     # train(skill_array, response_array, skill_response_array)\n",
    "#     train_on_batch(skill_array, response_array, skill_response_array)\n",
    "\n",
    "#     # python train.py -S skill_df_delft_15.csv -R response_df_delft_15.csv -dict skill_dict_delft_15.json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size== 64\n",
      "epoch size= 5\n",
      "input_dim_order 336\n",
      "input_dim 672\n",
      "DKTnet initialization done\n",
      "x_train.shape,y_train.shape\n",
      "(9242, 100) (9242, 100)\n",
      "using custom loss.....\n",
      "using custom loss.....\n",
      "Summary of the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, None, 672)         0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 672)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 40)          114080    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 336)         13776     \n",
      "=================================================================\n",
      "Total params: 127,856\n",
      "Trainable params: 127,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Data set shuffled, preparing for split\n",
      "train: 7394 test: 1848\n",
      "***Epoch 1 starts****\n",
      "336\n",
      "(7394, 100, 336) (7394, 100) (7394, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,accuracy 0.19400400205932813 0.9042993280394324\n",
      "iter: 116/145 done\n",
      "336\n",
      "(1848, 100, 336) (1848, 100) (1848, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, accuracy 0.16973008420960656 0.9171397809324593\n",
      "***Epoch 2 starts****\n",
      "336\n",
      "(7394, 100, 336) (7394, 100) (7394, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,accuracy 0.1469669115954432 0.934699624263007\n",
      "iter: 116/145 done\n",
      "336\n",
      "(1848, 100, 336) (1848, 100) (1848, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, accuracy 0.12416054154264516 0.9511353270760898\n",
      "***Epoch 3 starts****\n",
      "336\n",
      "(7394, 100, 336) (7394, 100) (7394, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,accuracy 0.10675928263186381 0.9638559412339638\n",
      "iter: 116/145 done\n",
      "336\n",
      "(1848, 100, 336) (1848, 100) (1848, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, accuracy 0.08669229391319998 0.975962129132501\n",
      "***Epoch 4 starts****\n",
      "336\n",
      "(7394, 100, 336) (7394, 100) (7394, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,accuracy 0.07395867839584062 0.9813076822922148\n",
      "iter: 116/145 done\n",
      "336\n",
      "(1848, 100, 336) (1848, 100) (1848, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, accuracy 0.05884035201422099 0.9867503129202744\n",
      "***Epoch 5 starts****\n",
      "336\n",
      "(7394, 100, 336) (7394, 100) (7394, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion training data result,loss,accuracy 0.051188100354167924 0.989073985609515\n",
      "iter: 116/145 done\n",
      "336\n",
      "(1848, 100, 336) (1848, 100) (1848, 100, 672)\n",
      "Training set shuffled\n",
      "Evalutaion validation data result, loss, accuracy 0.04040108505507995 0.9936853462252123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \"\"\"phase 1- delft 15\"\"\"\n",
    "\n",
    "data_dir1='code/delft15_phase1/'\n",
    "event_dict=('code/delft15_phase2/'+'all_event_dict_delft15.json')\n",
    "skill=os.path.join(data_dir1+\"problemevent_df_delft_15.csv\")\n",
    "response=os.path.join(data_dir1+\"problemresponse_df_delft_15.csv\")\n",
    "\n",
    "\n",
    "# subset for testing\n",
    "\n",
    "skill_matrix,response_array,problem_num, non_problem_event_num=load_data(skill,response,event_dict,problem_num=336)\n",
    "\n",
    "\n",
    "\n",
    "# skill_matrix=skill_matrix[:500,:]\n",
    "# response_array=response_array[:500,:]\n",
    "\n",
    "# # load preprocess in each batch, put in train_batch fynction\n",
    "# print('Number of non-problem events:', non_problem_event_num)\n",
    "# skill_array, response_array, skill_response_array=preprocess(skill_matrix,response_array,problem_num, non_problem_event_num)\n",
    "\n",
    "train_on_batch(skill_matrix,response_array,custom_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_batch(skill_array,response_array, skill_response_array,custom_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_batch(skill_array,response_array, skill_response_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# phase 1 : is_behaviour=FALSE\n",
    "skill_matrix,response_array,problem_num, non_problem_event_num=load_data(skill,response,event_dict,problem_num=336,is_behaviour=False)\n",
    "skill_matrix=skill_matrix[:1000,:]\n",
    "response_array=response_array[:1000,:]\n",
    "print('Number of non-problem events:', non_problem_event_num)\n",
    "skill_array, response_array, skill_response_array=preprocess(skill_matrix,response_array,problem_num, non_problem_event_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_batch(skill_array,response_array, skill_response_array,custom_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=np.array([0,0,0,0,10,15,6,7]).reshape(2,2,2)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "s=(K.argmax(k,axis=-1))\n",
    "s.eval().shape\n",
    "o=tf.one_hot(\n",
    "    s,\n",
    "    4,\n",
    "    on_value=3,\n",
    "    off_value=0,\n",
    "    axis=-1,\n",
    "    dtype=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean(K.sum(o,axis=-1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.sum(o,axis=-1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[0,0]\n",
    "s+=[1,2]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
